{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "<a id=\"top\"></a>\n",
    "# DS 7331 Data Mining: Lab 2 iPython Notebook\n",
    "Created On: February 11, 2019\n",
    "### Authors:  \n",
    "- Arora, Tanvi                \n",
    "- Chandna, Rajat\n",
    "- Henderson Kuns, Nicol\n",
    "- Ramasundaram, Kumar\n",
    "- Vasquez, James\n",
    "LRInterpertFeat\n",
    "\n",
    "# Logisitic Regression and Support Vector Machines\n",
    "\n",
    "## Contents\n",
    "* <a href=\"#DataPrep\">Data Prepping</a>\n",
    "    * <a href=\"#onehotencode\">One Hot Encoding</a>\n",
    "    * <a href=\"#Perform8020split\">Perform 80/20 split</a>  \n",
    "    * <a href=\"#PrepTestData\">Prep Test Data</a>    \n",
    "* <a href=\"#CreateLRModel\">Create Models</a>\n",
    "    * <a href=\"#CreateLRModel\">Simple Logistic Regression Model</a>  \n",
    "    * <a href=\"#LRGridSearch\">Grid Search</a>   \n",
    "    * <a href=\"#LRInterpertFeat\">Feature Interpertation</a>   \n",
    "* <a href=\"#SVMModel\">Simple SVM Model</a>\n",
    "    * <a href=\"#SVMRBF\">RBF Grid Search</a>   \n",
    "    * <a href=\"#SVMPOLY\">Poly Grid Search</a>   \n",
    "    * <a href=\"#SVMFINAL\">Final SVM Model on Validation Dataset</a>\n",
    "    * <a href=\"#SVMFINAL_Test\">Final SVM Model on Additional Test Dataset</a> \n",
    "* <a href=\"#MODELADV\">Model Advantages</a>\n",
    "* <a href=\"#INTVECT\">Interpret Support Vector</a>\n",
    "* <a href=\"#ECPWORK\">Exceptionnal Work</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"DataPrep\"></a>\n",
    "### Getting Dataset Ready for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# To display plots inside the iPython Notebook itself\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"age\";\"job\";\"marital\";\"education\";\"default\";\"balance\";\"housing\";\"loan\";\"contact\";\"day\";\"month\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"y\"\n",
      "\n",
      "58;\"management\";\"married\";\"tertiary\";\"no\";2143;\"yes\";\"no\";\"unknown\";5;\"may\";261;1;-1;0;\"unknown\";\"no\"\n",
      "\n",
      "44;\"technician\";\"single\";\"secondary\";\"no\";29;\"yes\";\"no\";\"unknown\";5;\"may\";151;1;-1;0;\"unknown\";\"no\"\n",
      "\n",
      "33;\"entrepreneur\";\"married\";\"secondary\";\"no\";2;\"yes\";\"yes\";\"unknown\";5;\"may\";76;1;-1;0;\"unknown\";\"no\"\n",
      "\n",
      "47;\"blue-collar\";\"married\";\"unknown\";\"no\";1506;\"yes\";\"no\";\"unknown\";5;\"may\";92;1;-1;0;\"unknown\";\"no\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To verify how data is orgainzed in file(to find the delimiter) and then\n",
    "# use corresponding function to open the file. eg\n",
    "# data could be in .csv. .tsv, excel format etc.\n",
    "pathOfDataFile = \"data/bank-full.csv\"\n",
    "firstFewLines = list()\n",
    "noOfLinesToView = 5\n",
    "\n",
    "with open(pathOfDataFile) as dataFile:\n",
    "    firstFewLines = [next(dataFile) for i in range(noOfLinesToView)]\n",
    "    for line in firstFewLines:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>Subscribed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>231</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>447</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "5   35    management  married   tertiary      no      231     yes   no   \n",
       "6   28    management   single   tertiary      no      447     yes  yes   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome Subscribed  \n",
       "0  unknown    5   may       261         1     -1         0  unknown         no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown         no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown         no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown         no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown         no  \n",
       "5  unknown    5   may       139         1     -1         0  unknown         no  \n",
       "6  unknown    5   may       217         1     -1         0  unknown         no  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the semi-colon delimited data file into pandas dataFrame\n",
    "bankPromo_df = pd.read_csv(pathOfDataFile, sep = \";\")\n",
    "\n",
    "# Rename the Target/Final Outcome column from \"y\" to \"Subscribed\" as based on data description.\n",
    "bankPromo_df = bankPromo_df.rename(columns={\"y\":\"Subscribed\"})\n",
    "\n",
    "bankPromo_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      "age           45211 non-null int64\n",
      "job           45211 non-null object\n",
      "marital       45211 non-null object\n",
      "education     45211 non-null object\n",
      "default       45211 non-null object\n",
      "balance       45211 non-null int64\n",
      "housing       45211 non-null object\n",
      "loan          45211 non-null object\n",
      "contact       45211 non-null object\n",
      "day           45211 non-null int64\n",
      "month         45211 non-null object\n",
      "duration      45211 non-null int64\n",
      "campaign      45211 non-null int64\n",
      "pdays         45211 non-null int64\n",
      "previous      45211 non-null int64\n",
      "poutcome      45211 non-null object\n",
      "Subscribed    45211 non-null object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 5.9+ MB\n"
     ]
    }
   ],
   "source": [
    "bankPromo_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique values(Levels) for categorical variables.\n",
    "# List to hold names of categorical variables\n",
    "categoricalVars = list()\n",
    "# List to hold names of numerical variables\n",
    "numericalVars = list()\n",
    "\n",
    "for colName in bankPromo_df.columns:\n",
    "    if bankPromo_df[colName].dtype == np.int64:\n",
    "        numericalVars.append(colName)\n",
    "    elif bankPromo_df[colName].dtype == np.object:\n",
    "        categoricalVars.append(colName)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Remove Target column from final categorical Var list\n",
    "categoricalVars.remove('Subscribed')\n",
    "\n",
    "print(numericalVars)\n",
    "print(categoricalVars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"onehotencode\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Perform One Hot Encoding for categorical variables in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of original data frame\n",
    "bankPromoModel_Df = bankPromo_df.copy()\n",
    "bankPromoModel_Df['Target'] = bankPromoModel_Df['Subscribed'].apply(lambda resp : 1 if resp == \"yes\" else 0)\n",
    "bankPromoModel_Df['Target'] = bankPromoModel_Df['Target'].astype(np.int)\n",
    "# Delete the original 'Subscribed' column\n",
    "del bankPromoModel_Df['Subscribed']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the pDays feature as it had high correlation with \"previous\" feature\n",
    "del bankPromoModel_Df['pdays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 42 columns):\n",
      "age                    45211 non-null int64\n",
      "balance                45211 non-null int64\n",
      "day                    45211 non-null int64\n",
      "duration               45211 non-null int64\n",
      "campaign               45211 non-null int64\n",
      "previous               45211 non-null int64\n",
      "Target                 45211 non-null int64\n",
      "job_blue-collar        45211 non-null uint8\n",
      "job_entrepreneur       45211 non-null uint8\n",
      "job_housemaid          45211 non-null uint8\n",
      "job_management         45211 non-null uint8\n",
      "job_retired            45211 non-null uint8\n",
      "job_self-employed      45211 non-null uint8\n",
      "job_services           45211 non-null uint8\n",
      "job_student            45211 non-null uint8\n",
      "job_technician         45211 non-null uint8\n",
      "job_unemployed         45211 non-null uint8\n",
      "job_unknown            45211 non-null uint8\n",
      "marital_married        45211 non-null uint8\n",
      "marital_single         45211 non-null uint8\n",
      "education_secondary    45211 non-null uint8\n",
      "education_tertiary     45211 non-null uint8\n",
      "education_unknown      45211 non-null uint8\n",
      "default_yes            45211 non-null uint8\n",
      "housing_yes            45211 non-null uint8\n",
      "loan_yes               45211 non-null uint8\n",
      "contact_telephone      45211 non-null uint8\n",
      "contact_unknown        45211 non-null uint8\n",
      "month_aug              45211 non-null uint8\n",
      "month_dec              45211 non-null uint8\n",
      "month_feb              45211 non-null uint8\n",
      "month_jan              45211 non-null uint8\n",
      "month_jul              45211 non-null uint8\n",
      "month_jun              45211 non-null uint8\n",
      "month_mar              45211 non-null uint8\n",
      "month_may              45211 non-null uint8\n",
      "month_nov              45211 non-null uint8\n",
      "month_oct              45211 non-null uint8\n",
      "month_sep              45211 non-null uint8\n",
      "poutcome_other         45211 non-null uint8\n",
      "poutcome_success       45211 non-null uint8\n",
      "poutcome_unknown       45211 non-null uint8\n",
      "dtypes: int64(7), uint8(35)\n",
      "memory usage: 3.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Covert all categorical variables to corresponding indicator variables\n",
    "for categoricalVar in categoricalVars:\n",
    "    tmpDf = pd.DataFrame()\n",
    "    # Remove 1st class level to avoid multicollinearity\n",
    "    tmpDf = pd.get_dummies(bankPromoModel_Df[categoricalVar], prefix=categoricalVar, drop_first=True)\n",
    "    bankPromoModel_Df = pd.concat((bankPromoModel_Df, tmpDf), axis=1)\n",
    "\n",
    "# Now remove the original categorical vars since indicator variables are created from them.\n",
    "bankPromoModel_Df.drop(categoricalVars, inplace=True, axis=1)\n",
    "bankPromoModel_Df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "________________________________________________________________________________________________________\n",
    "<a id=\"Perform8020split\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Create 10 Splits Stratified Cross Validation Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=10, random_state=999, test_size=0.2,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "# Training and Test Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if 'Target' in bankPromoModel_Df:\n",
    "    y = bankPromoModel_Df['Target'].values # get the labels we want\n",
    "    del bankPromoModel_Df['Target']        # get rid of the class label\n",
    "    X = bankPromoModel_Df.values           # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# To use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 10\n",
    "stratified_cv_object = StratifiedShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2, random_state=999)\n",
    "                         \n",
    "print(stratified_cv_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=999, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "# Training and Test Split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "if 'Target' in bankPromoModel_Df:\n",
    "    y = bankPromoModel_Df['Target'].values # get the labels we want\n",
    "    del bankPromoModel_Df['Target']        # get rid of the class label\n",
    "    X = bankPromoModel_Df.values           # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# To use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 10\n",
    "stratifiedKfold_cv_object = StratifiedKFold(n_splits=num_cv_iterations, random_state=999)\n",
    "                         \n",
    "print(stratifiedKfold_cv_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"PrepTestData\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Getting ready Additional Test Dataset(with 10% instances) for final model fitting and evaluations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 17 columns):\n",
      "age           4521 non-null int64\n",
      "job           4521 non-null object\n",
      "marital       4521 non-null object\n",
      "education     4521 non-null object\n",
      "default       4521 non-null object\n",
      "balance       4521 non-null int64\n",
      "housing       4521 non-null object\n",
      "loan          4521 non-null object\n",
      "contact       4521 non-null object\n",
      "day           4521 non-null int64\n",
      "month         4521 non-null object\n",
      "duration      4521 non-null int64\n",
      "campaign      4521 non-null int64\n",
      "pdays         4521 non-null int64\n",
      "previous      4521 non-null int64\n",
      "poutcome      4521 non-null object\n",
      "Subscribed    4521 non-null object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 600.5+ KB\n"
     ]
    }
   ],
   "source": [
    "pathOfAdditionalDataFile = \"data/bank.csv\"\n",
    "\n",
    "# Import the semi-colon delimited data file into pandas dataFrame\n",
    "bankPromoAdditional_df = pd.read_csv(pathOfAdditionalDataFile, sep = \";\")\n",
    "\n",
    "# Rename the Target/Final Outcome column from \"y\" to \"Subscribed\" as based on data description.\n",
    "bankPromoAdditional_df = bankPromoAdditional_df.rename(columns={\"y\":\"Subscribed\"})\n",
    "\n",
    "bankPromoAdditional_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankPromoAdditional_df['Target'] = bankPromoAdditional_df['Subscribed'].apply(lambda resp : 1 if resp == \"yes\" else 0)\n",
    "bankPromoAdditional_df['Target'] = bankPromoAdditional_df['Target'].astype(np.int)\n",
    "# Delete the original 'Subscribed' column\n",
    "del bankPromoAdditional_df['Subscribed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove pDays\n",
    "del bankPromoAdditional_df['pdays']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 41 columns):\n",
      "age                    4521 non-null int64\n",
      "balance                4521 non-null int64\n",
      "day                    4521 non-null int64\n",
      "duration               4521 non-null int64\n",
      "campaign               4521 non-null int64\n",
      "previous               4521 non-null int64\n",
      "job_blue-collar        4521 non-null uint8\n",
      "job_entrepreneur       4521 non-null uint8\n",
      "job_housemaid          4521 non-null uint8\n",
      "job_management         4521 non-null uint8\n",
      "job_retired            4521 non-null uint8\n",
      "job_self-employed      4521 non-null uint8\n",
      "job_services           4521 non-null uint8\n",
      "job_student            4521 non-null uint8\n",
      "job_technician         4521 non-null uint8\n",
      "job_unemployed         4521 non-null uint8\n",
      "job_unknown            4521 non-null uint8\n",
      "marital_married        4521 non-null uint8\n",
      "marital_single         4521 non-null uint8\n",
      "education_secondary    4521 non-null uint8\n",
      "education_tertiary     4521 non-null uint8\n",
      "education_unknown      4521 non-null uint8\n",
      "default_yes            4521 non-null uint8\n",
      "housing_yes            4521 non-null uint8\n",
      "loan_yes               4521 non-null uint8\n",
      "contact_telephone      4521 non-null uint8\n",
      "contact_unknown        4521 non-null uint8\n",
      "month_aug              4521 non-null uint8\n",
      "month_dec              4521 non-null uint8\n",
      "month_feb              4521 non-null uint8\n",
      "month_jan              4521 non-null uint8\n",
      "month_jul              4521 non-null uint8\n",
      "month_jun              4521 non-null uint8\n",
      "month_mar              4521 non-null uint8\n",
      "month_may              4521 non-null uint8\n",
      "month_nov              4521 non-null uint8\n",
      "month_oct              4521 non-null uint8\n",
      "month_sep              4521 non-null uint8\n",
      "poutcome_other         4521 non-null uint8\n",
      "poutcome_success       4521 non-null uint8\n",
      "poutcome_unknown       4521 non-null uint8\n",
      "dtypes: int64(6), uint8(35)\n",
      "memory usage: 366.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Covert all categorical variables to corresponding indicator variables\n",
    "for categoricalVar in categoricalVars:\n",
    "    tmpDf = pd.DataFrame()\n",
    "    # Remove 1st class level to avoid multicollinearity\n",
    "    tmpDf = pd.get_dummies(bankPromoAdditional_df[categoricalVar], prefix=categoricalVar, drop_first=True)\n",
    "    bankPromoAdditional_df = pd.concat((bankPromoAdditional_df, tmpDf), axis=1)\n",
    "\n",
    "# Now remove the original categorical vars since indicator variables are created from them.\n",
    "bankPromoAdditional_df.drop(categoricalVars, inplace=True, axis=1)\n",
    "\n",
    "if 'Target' in bankPromoAdditional_df:\n",
    "    y_Final = bankPromoAdditional_df['Target'].values # get the labels we want\n",
    "    del bankPromoAdditional_df['Target']        # get rid of the class label\n",
    "    X_Final = bankPromoAdditional_df.values\n",
    "\n",
    "bankPromoAdditional_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"CreateLRModel\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Create Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"SVMModel\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Simple SVM Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.155095</td>\n",
       "      <td>34.151568</td>\n",
       "      <td>0.442489</td>\n",
       "      <td>0.536374</td>\n",
       "      <td>0.905848</td>\n",
       "      <td>0.943094</td>\n",
       "      <td>0.901913</td>\n",
       "      <td>0.917358</td>\n",
       "      <td>0.660413</td>\n",
       "      <td>0.780235</td>\n",
       "      <td>0.332703</td>\n",
       "      <td>0.408650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68.047714</td>\n",
       "      <td>34.066402</td>\n",
       "      <td>0.409762</td>\n",
       "      <td>0.533934</td>\n",
       "      <td>0.908474</td>\n",
       "      <td>0.942348</td>\n",
       "      <td>0.898374</td>\n",
       "      <td>0.917026</td>\n",
       "      <td>0.639279</td>\n",
       "      <td>0.778533</td>\n",
       "      <td>0.301512</td>\n",
       "      <td>0.406287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71.070259</td>\n",
       "      <td>34.972438</td>\n",
       "      <td>0.460468</td>\n",
       "      <td>0.522341</td>\n",
       "      <td>0.910840</td>\n",
       "      <td>0.942594</td>\n",
       "      <td>0.905673</td>\n",
       "      <td>0.916058</td>\n",
       "      <td>0.695985</td>\n",
       "      <td>0.781176</td>\n",
       "      <td>0.344045</td>\n",
       "      <td>0.392342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68.945323</td>\n",
       "      <td>36.137710</td>\n",
       "      <td>0.442467</td>\n",
       "      <td>0.534931</td>\n",
       "      <td>0.903748</td>\n",
       "      <td>0.942832</td>\n",
       "      <td>0.903019</td>\n",
       "      <td>0.917358</td>\n",
       "      <td>0.675728</td>\n",
       "      <td>0.782787</td>\n",
       "      <td>0.328922</td>\n",
       "      <td>0.406287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.218162</td>\n",
       "      <td>35.670571</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.526694</td>\n",
       "      <td>0.905101</td>\n",
       "      <td>0.943542</td>\n",
       "      <td>0.901250</td>\n",
       "      <td>0.916169</td>\n",
       "      <td>0.648115</td>\n",
       "      <td>0.775632</td>\n",
       "      <td>0.341210</td>\n",
       "      <td>0.398724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>72.355329</td>\n",
       "      <td>35.120178</td>\n",
       "      <td>0.443450</td>\n",
       "      <td>0.525689</td>\n",
       "      <td>0.911227</td>\n",
       "      <td>0.940267</td>\n",
       "      <td>0.903682</td>\n",
       "      <td>0.916280</td>\n",
       "      <td>0.684418</td>\n",
       "      <td>0.779378</td>\n",
       "      <td>0.327977</td>\n",
       "      <td>0.396597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>68.841946</td>\n",
       "      <td>35.751309</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.535808</td>\n",
       "      <td>0.900787</td>\n",
       "      <td>0.944734</td>\n",
       "      <td>0.901471</td>\n",
       "      <td>0.917026</td>\n",
       "      <td>0.661509</td>\n",
       "      <td>0.775291</td>\n",
       "      <td>0.323251</td>\n",
       "      <td>0.409359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68.889043</td>\n",
       "      <td>34.649703</td>\n",
       "      <td>0.431423</td>\n",
       "      <td>0.527187</td>\n",
       "      <td>0.909024</td>\n",
       "      <td>0.941691</td>\n",
       "      <td>0.902355</td>\n",
       "      <td>0.916335</td>\n",
       "      <td>0.676768</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.316635</td>\n",
       "      <td>0.398724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41.310634</td>\n",
       "      <td>24.692160</td>\n",
       "      <td>0.458831</td>\n",
       "      <td>0.529183</td>\n",
       "      <td>0.901194</td>\n",
       "      <td>0.943467</td>\n",
       "      <td>0.904788</td>\n",
       "      <td>0.916363</td>\n",
       "      <td>0.684803</td>\n",
       "      <td>0.774840</td>\n",
       "      <td>0.344991</td>\n",
       "      <td>0.401796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45.715505</td>\n",
       "      <td>25.757265</td>\n",
       "      <td>0.434069</td>\n",
       "      <td>0.529936</td>\n",
       "      <td>0.901098</td>\n",
       "      <td>0.942322</td>\n",
       "      <td>0.900807</td>\n",
       "      <td>0.916860</td>\n",
       "      <td>0.652751</td>\n",
       "      <td>0.782548</td>\n",
       "      <td>0.325142</td>\n",
       "      <td>0.400615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  69.155095   34.151568       0.442489        0.536374  0.905848   0.943094   \n",
       "1  68.047714   34.066402       0.409762        0.533934  0.908474   0.942348   \n",
       "2  71.070259   34.972438       0.460468        0.522341  0.910840   0.942594   \n",
       "3  68.945323   36.137710       0.442467        0.534931  0.903748   0.942832   \n",
       "4  68.218162   35.670571       0.447059        0.526694  0.905101   0.943542   \n",
       "5  72.355329   35.120178       0.443450        0.525689  0.911227   0.940267   \n",
       "6  68.841946   35.751309       0.434286        0.535808  0.900787   0.944734   \n",
       "7  68.889043   34.649703       0.431423        0.527187  0.909024   0.941691   \n",
       "8  41.310634   24.692160       0.458831        0.529183  0.901194   0.943467   \n",
       "9  45.715505   25.757265       0.434069        0.529936  0.901098   0.942322   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.901913        0.917358        0.660413         0.780235   \n",
       "1       0.898374        0.917026        0.639279         0.778533   \n",
       "2       0.905673        0.916058        0.695985         0.781176   \n",
       "3       0.903019        0.917358        0.675728         0.782787   \n",
       "4       0.901250        0.916169        0.648115         0.775632   \n",
       "5       0.903682        0.916280        0.684418         0.779378   \n",
       "6       0.901471        0.917026        0.661509         0.775291   \n",
       "7       0.902355        0.916335        0.676768         0.777778   \n",
       "8       0.904788        0.916363        0.684803         0.774840   \n",
       "9       0.900807        0.916860        0.652751         0.782548   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.332703      0.408650  \n",
       "1     0.301512      0.406287  \n",
       "2     0.344045      0.392342  \n",
       "3     0.328922      0.406287  \n",
       "4     0.341210      0.398724  \n",
       "5     0.327977      0.396597  \n",
       "6     0.323251      0.409359  \n",
       "7     0.316635      0.398724  \n",
       "8     0.344991      0.401796  \n",
       "9     0.325142      0.400615  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>82.682091</td>\n",
       "      <td>19.503515</td>\n",
       "      <td>0.026119</td>\n",
       "      <td>0.540348</td>\n",
       "      <td>0.889240</td>\n",
       "      <td>0.944899</td>\n",
       "      <td>0.884564</td>\n",
       "      <td>0.917545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.776684</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.414286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.079305</td>\n",
       "      <td>19.230631</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.546727</td>\n",
       "      <td>0.370369</td>\n",
       "      <td>0.942571</td>\n",
       "      <td>0.819770</td>\n",
       "      <td>0.918823</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.788287</td>\n",
       "      <td>0.005671</td>\n",
       "      <td>0.418487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89.725657</td>\n",
       "      <td>20.189960</td>\n",
       "      <td>0.051690</td>\n",
       "      <td>0.559631</td>\n",
       "      <td>0.455206</td>\n",
       "      <td>0.945343</td>\n",
       "      <td>0.788985</td>\n",
       "      <td>0.920324</td>\n",
       "      <td>0.054507</td>\n",
       "      <td>0.791699</td>\n",
       "      <td>0.049149</td>\n",
       "      <td>0.432773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84.717636</td>\n",
       "      <td>19.972225</td>\n",
       "      <td>0.050304</td>\n",
       "      <td>0.569840</td>\n",
       "      <td>0.393581</td>\n",
       "      <td>0.947252</td>\n",
       "      <td>0.757797</td>\n",
       "      <td>0.922045</td>\n",
       "      <td>0.046474</td>\n",
       "      <td>0.803749</td>\n",
       "      <td>0.054820</td>\n",
       "      <td>0.441387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91.924017</td>\n",
       "      <td>19.718217</td>\n",
       "      <td>0.144304</td>\n",
       "      <td>0.555252</td>\n",
       "      <td>0.580249</td>\n",
       "      <td>0.942971</td>\n",
       "      <td>0.850476</td>\n",
       "      <td>0.920079</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>0.107750</td>\n",
       "      <td>0.426471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>87.035394</td>\n",
       "      <td>20.133172</td>\n",
       "      <td>0.197415</td>\n",
       "      <td>0.559253</td>\n",
       "      <td>0.503552</td>\n",
       "      <td>0.944690</td>\n",
       "      <td>0.848927</td>\n",
       "      <td>0.919931</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.785334</td>\n",
       "      <td>0.158790</td>\n",
       "      <td>0.434244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.846771</td>\n",
       "      <td>20.512317</td>\n",
       "      <td>0.166983</td>\n",
       "      <td>0.553717</td>\n",
       "      <td>0.575662</td>\n",
       "      <td>0.946234</td>\n",
       "      <td>0.805795</td>\n",
       "      <td>0.919145</td>\n",
       "      <td>0.167619</td>\n",
       "      <td>0.781394</td>\n",
       "      <td>0.166352</td>\n",
       "      <td>0.428782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>86.296850</td>\n",
       "      <td>17.631637</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.618125</td>\n",
       "      <td>0.208879</td>\n",
       "      <td>0.955003</td>\n",
       "      <td>0.398806</td>\n",
       "      <td>0.927820</td>\n",
       "      <td>0.019746</td>\n",
       "      <td>0.810986</td>\n",
       "      <td>0.085066</td>\n",
       "      <td>0.499370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62.521152</td>\n",
       "      <td>13.787464</td>\n",
       "      <td>0.260456</td>\n",
       "      <td>0.543772</td>\n",
       "      <td>0.609869</td>\n",
       "      <td>0.942091</td>\n",
       "      <td>0.827914</td>\n",
       "      <td>0.918801</td>\n",
       "      <td>0.261950</td>\n",
       "      <td>0.793312</td>\n",
       "      <td>0.258979</td>\n",
       "      <td>0.413655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45.567088</td>\n",
       "      <td>11.538848</td>\n",
       "      <td>0.292779</td>\n",
       "      <td>0.660222</td>\n",
       "      <td>0.723169</td>\n",
       "      <td>0.954828</td>\n",
       "      <td>0.525442</td>\n",
       "      <td>0.935219</td>\n",
       "      <td>0.177246</td>\n",
       "      <td>0.854521</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.537912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0   82.682091   19.503515       0.026119        0.540348  0.889240   0.944899   \n",
       "1  102.079305   19.230631       0.007308        0.546727  0.370369   0.942571   \n",
       "2   89.725657   20.189960       0.051690        0.559631  0.455206   0.945343   \n",
       "3   84.717636   19.972225       0.050304        0.569840  0.393581   0.947252   \n",
       "4   91.924017   19.718217       0.144304        0.555252  0.580249   0.942971   \n",
       "5   87.035394   20.133172       0.197415        0.559253  0.503552   0.944690   \n",
       "6   86.846771   20.512317       0.166983        0.553717  0.575662   0.946234   \n",
       "7   86.296850   17.631637       0.032051        0.618125  0.208879   0.955003   \n",
       "8   62.521152   13.787464       0.260456        0.543772  0.609869   0.942091   \n",
       "9   45.567088   11.538848       0.292779        0.660222  0.723169   0.954828   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.884564        0.917545        1.000000         0.776684   \n",
       "1       0.819770        0.918823        0.010274         0.788287   \n",
       "2       0.788985        0.920324        0.054507         0.791699   \n",
       "3       0.757797        0.922045        0.046474         0.803749   \n",
       "4       0.850476        0.920079        0.218391         0.795455   \n",
       "5       0.848927        0.919931        0.260870         0.785334   \n",
       "6       0.805795        0.919145        0.167619         0.781394   \n",
       "7       0.398806        0.927820        0.019746         0.810986   \n",
       "8       0.827914        0.918801        0.261950         0.793312   \n",
       "9       0.525442        0.935219        0.177246         0.854521   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.013233      0.414286  \n",
       "1     0.005671      0.418487  \n",
       "2     0.049149      0.432773  \n",
       "3     0.054820      0.441387  \n",
       "4     0.107750      0.426471  \n",
       "5     0.158790      0.434244  \n",
       "6     0.166352      0.428782  \n",
       "7     0.085066      0.499370  \n",
       "8     0.258979      0.413655  \n",
       "9     0.840909      0.537912  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "scoring = {'F1_Score': 'f1', 'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'Precision': 'precision', \\\n",
    "          'Recall': 'recall'}\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run SVM model\n",
    "\n",
    "svmModel = make_pipeline(StandardScaler(), SVC(C=1.0, kernel='rbf', degree=3 , gamma='auto', random_state=999))\n",
    "scores = cross_validate(svmModel, X, y=y, cv=stratified_cv_object, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "print()\n",
    "display(pd.DataFrame(scores))\n",
    "\n",
    "scores = cross_validate(svmModel, X, y=y, cv=stratifiedKfold_cv_object, n_jobs=-1, scoring=scoring)\n",
    "display(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>139.271351</td>\n",
       "      <td>57.901207</td>\n",
       "      <td>0.565231</td>\n",
       "      <td>0.618541</td>\n",
       "      <td>0.917354</td>\n",
       "      <td>0.951077</td>\n",
       "      <td>0.842641</td>\n",
       "      <td>0.865295</td>\n",
       "      <td>0.417607</td>\n",
       "      <td>0.462475</td>\n",
       "      <td>0.874291</td>\n",
       "      <td>0.933585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126.176060</td>\n",
       "      <td>58.790069</td>\n",
       "      <td>0.574226</td>\n",
       "      <td>0.615540</td>\n",
       "      <td>0.918856</td>\n",
       "      <td>0.950382</td>\n",
       "      <td>0.850934</td>\n",
       "      <td>0.863470</td>\n",
       "      <td>0.431214</td>\n",
       "      <td>0.458957</td>\n",
       "      <td>0.859168</td>\n",
       "      <td>0.934294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125.510220</td>\n",
       "      <td>58.099394</td>\n",
       "      <td>0.563450</td>\n",
       "      <td>0.614334</td>\n",
       "      <td>0.923069</td>\n",
       "      <td>0.949960</td>\n",
       "      <td>0.844410</td>\n",
       "      <td>0.862530</td>\n",
       "      <td>0.419400</td>\n",
       "      <td>0.457222</td>\n",
       "      <td>0.858223</td>\n",
       "      <td>0.935949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140.566196</td>\n",
       "      <td>56.779839</td>\n",
       "      <td>0.569612</td>\n",
       "      <td>0.616824</td>\n",
       "      <td>0.922687</td>\n",
       "      <td>0.949735</td>\n",
       "      <td>0.850271</td>\n",
       "      <td>0.864106</td>\n",
       "      <td>0.429119</td>\n",
       "      <td>0.460214</td>\n",
       "      <td>0.846881</td>\n",
       "      <td>0.935004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133.407483</td>\n",
       "      <td>59.049373</td>\n",
       "      <td>0.557196</td>\n",
       "      <td>0.617228</td>\n",
       "      <td>0.916606</td>\n",
       "      <td>0.950806</td>\n",
       "      <td>0.840761</td>\n",
       "      <td>0.863996</td>\n",
       "      <td>0.412944</td>\n",
       "      <td>0.460093</td>\n",
       "      <td>0.856333</td>\n",
       "      <td>0.937367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>140.112008</td>\n",
       "      <td>58.834010</td>\n",
       "      <td>0.575558</td>\n",
       "      <td>0.616045</td>\n",
       "      <td>0.926412</td>\n",
       "      <td>0.949458</td>\n",
       "      <td>0.850603</td>\n",
       "      <td>0.863968</td>\n",
       "      <td>0.431059</td>\n",
       "      <td>0.459863</td>\n",
       "      <td>0.865784</td>\n",
       "      <td>0.932876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>139.358935</td>\n",
       "      <td>55.750773</td>\n",
       "      <td>0.565765</td>\n",
       "      <td>0.621253</td>\n",
       "      <td>0.912692</td>\n",
       "      <td>0.951827</td>\n",
       "      <td>0.847396</td>\n",
       "      <td>0.866567</td>\n",
       "      <td>0.424057</td>\n",
       "      <td>0.465045</td>\n",
       "      <td>0.849716</td>\n",
       "      <td>0.935476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>125.869808</td>\n",
       "      <td>57.201792</td>\n",
       "      <td>0.569221</td>\n",
       "      <td>0.616609</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.949332</td>\n",
       "      <td>0.845848</td>\n",
       "      <td>0.864189</td>\n",
       "      <td>0.422865</td>\n",
       "      <td>0.460319</td>\n",
       "      <td>0.870510</td>\n",
       "      <td>0.933585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>84.493445</td>\n",
       "      <td>38.988364</td>\n",
       "      <td>0.567192</td>\n",
       "      <td>0.617201</td>\n",
       "      <td>0.918168</td>\n",
       "      <td>0.950508</td>\n",
       "      <td>0.848280</td>\n",
       "      <td>0.863775</td>\n",
       "      <td>0.425663</td>\n",
       "      <td>0.459722</td>\n",
       "      <td>0.849716</td>\n",
       "      <td>0.938785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>76.965573</td>\n",
       "      <td>39.113493</td>\n",
       "      <td>0.561512</td>\n",
       "      <td>0.619668</td>\n",
       "      <td>0.919971</td>\n",
       "      <td>0.950812</td>\n",
       "      <td>0.843525</td>\n",
       "      <td>0.865904</td>\n",
       "      <td>0.417704</td>\n",
       "      <td>0.463678</td>\n",
       "      <td>0.856333</td>\n",
       "      <td>0.933822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  139.271351   57.901207       0.565231        0.618541  0.917354   0.951077   \n",
       "1  126.176060   58.790069       0.574226        0.615540  0.918856   0.950382   \n",
       "2  125.510220   58.099394       0.563450        0.614334  0.923069   0.949960   \n",
       "3  140.566196   56.779839       0.569612        0.616824  0.922687   0.949735   \n",
       "4  133.407483   59.049373       0.557196        0.617228  0.916606   0.950806   \n",
       "5  140.112008   58.834010       0.575558        0.616045  0.926412   0.949458   \n",
       "6  139.358935   55.750773       0.565765        0.621253  0.912692   0.951827   \n",
       "7  125.869808   57.201792       0.569221        0.616609  0.922874   0.949332   \n",
       "8   84.493445   38.988364       0.567192        0.617201  0.918168   0.950508   \n",
       "9   76.965573   39.113493       0.561512        0.619668  0.919971   0.950812   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.842641        0.865295        0.417607         0.462475   \n",
       "1       0.850934        0.863470        0.431214         0.458957   \n",
       "2       0.844410        0.862530        0.419400         0.457222   \n",
       "3       0.850271        0.864106        0.429119         0.460214   \n",
       "4       0.840761        0.863996        0.412944         0.460093   \n",
       "5       0.850603        0.863968        0.431059         0.459863   \n",
       "6       0.847396        0.866567        0.424057         0.465045   \n",
       "7       0.845848        0.864189        0.422865         0.460319   \n",
       "8       0.848280        0.863775        0.425663         0.459722   \n",
       "9       0.843525        0.865904        0.417704         0.463678   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.874291      0.933585  \n",
       "1     0.859168      0.934294  \n",
       "2     0.858223      0.935949  \n",
       "3     0.846881      0.935004  \n",
       "4     0.856333      0.937367  \n",
       "5     0.865784      0.932876  \n",
       "6     0.849716      0.935476  \n",
       "7     0.870510      0.933585  \n",
       "8     0.849716      0.938785  \n",
       "9     0.856333      0.933822  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157.591276</td>\n",
       "      <td>33.426260</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.616786</td>\n",
       "      <td>0.946716</td>\n",
       "      <td>0.951429</td>\n",
       "      <td>0.893852</td>\n",
       "      <td>0.863550</td>\n",
       "      <td>0.945455</td>\n",
       "      <td>0.459293</td>\n",
       "      <td>0.098299</td>\n",
       "      <td>0.938655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>161.131280</td>\n",
       "      <td>33.369392</td>\n",
       "      <td>0.307592</td>\n",
       "      <td>0.619467</td>\n",
       "      <td>0.712601</td>\n",
       "      <td>0.951644</td>\n",
       "      <td>0.766033</td>\n",
       "      <td>0.866057</td>\n",
       "      <td>0.235235</td>\n",
       "      <td>0.463920</td>\n",
       "      <td>0.444234</td>\n",
       "      <td>0.931933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171.182007</td>\n",
       "      <td>31.114848</td>\n",
       "      <td>0.113909</td>\n",
       "      <td>0.625308</td>\n",
       "      <td>0.390318</td>\n",
       "      <td>0.953338</td>\n",
       "      <td>0.673081</td>\n",
       "      <td>0.869378</td>\n",
       "      <td>0.083406</td>\n",
       "      <td>0.470557</td>\n",
       "      <td>0.179584</td>\n",
       "      <td>0.931723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152.998072</td>\n",
       "      <td>32.387388</td>\n",
       "      <td>0.262554</td>\n",
       "      <td>0.624333</td>\n",
       "      <td>0.608382</td>\n",
       "      <td>0.954546</td>\n",
       "      <td>0.587481</td>\n",
       "      <td>0.868420</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.468710</td>\n",
       "      <td>0.627599</td>\n",
       "      <td>0.934664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157.229172</td>\n",
       "      <td>31.904568</td>\n",
       "      <td>0.297000</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>0.701712</td>\n",
       "      <td>0.953522</td>\n",
       "      <td>0.689007</td>\n",
       "      <td>0.866405</td>\n",
       "      <td>0.201903</td>\n",
       "      <td>0.464806</td>\n",
       "      <td>0.561437</td>\n",
       "      <td>0.937815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>172.406284</td>\n",
       "      <td>31.522299</td>\n",
       "      <td>0.254057</td>\n",
       "      <td>0.623123</td>\n",
       "      <td>0.605245</td>\n",
       "      <td>0.952728</td>\n",
       "      <td>0.644105</td>\n",
       "      <td>0.867412</td>\n",
       "      <td>0.168305</td>\n",
       "      <td>0.466771</td>\n",
       "      <td>0.517958</td>\n",
       "      <td>0.936975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>173.362316</td>\n",
       "      <td>31.588564</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.628616</td>\n",
       "      <td>0.952800</td>\n",
       "      <td>0.575315</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.154839</td>\n",
       "      <td>0.464942</td>\n",
       "      <td>0.589792</td>\n",
       "      <td>0.940336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>135.526077</td>\n",
       "      <td>27.819915</td>\n",
       "      <td>0.109536</td>\n",
       "      <td>0.649156</td>\n",
       "      <td>0.256386</td>\n",
       "      <td>0.960526</td>\n",
       "      <td>0.291528</td>\n",
       "      <td>0.880511</td>\n",
       "      <td>0.064211</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.372401</td>\n",
       "      <td>0.944958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>121.960758</td>\n",
       "      <td>22.050131</td>\n",
       "      <td>0.380363</td>\n",
       "      <td>0.613800</td>\n",
       "      <td>0.792081</td>\n",
       "      <td>0.951385</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.862718</td>\n",
       "      <td>0.248268</td>\n",
       "      <td>0.457440</td>\n",
       "      <td>0.812854</td>\n",
       "      <td>0.932563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>91.075434</td>\n",
       "      <td>19.310992</td>\n",
       "      <td>0.266058</td>\n",
       "      <td>0.686054</td>\n",
       "      <td>0.685050</td>\n",
       "      <td>0.963673</td>\n",
       "      <td>0.395796</td>\n",
       "      <td>0.899978</td>\n",
       "      <td>0.155027</td>\n",
       "      <td>0.542119</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.934047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  157.591276   33.426260       0.178082        0.616786  0.946716   0.951429   \n",
       "1  161.131280   33.369392       0.307592        0.619467  0.712601   0.951644   \n",
       "2  171.182007   31.114848       0.113909        0.625308  0.390318   0.953338   \n",
       "3  152.998072   32.387388       0.262554        0.624333  0.608382   0.954546   \n",
       "4  157.229172   31.904568       0.297000        0.621554  0.701712   0.953522   \n",
       "5  172.406284   31.522299       0.254057        0.623123  0.605245   0.952728   \n",
       "6  173.362316   31.588564       0.245283        0.622228  0.628616   0.952800   \n",
       "7  135.526077   27.819915       0.109536        0.649156  0.256386   0.960526   \n",
       "8  121.960758   22.050131       0.380363        0.613800  0.792081   0.951385   \n",
       "9   91.075434   19.310992       0.266058        0.686054  0.685050   0.963673   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.893852        0.863550        0.945455         0.459293   \n",
       "1       0.766033        0.866057        0.235235         0.463920   \n",
       "2       0.673081        0.869378        0.083406         0.470557   \n",
       "3       0.587481        0.868420        0.166000         0.468710   \n",
       "4       0.689007        0.866405        0.201903         0.464806   \n",
       "5       0.644105        0.867412        0.168305         0.466771   \n",
       "6       0.575315        0.866429        0.154839         0.464942   \n",
       "7       0.291528        0.880511        0.064211         0.494394   \n",
       "8       0.690113        0.862718        0.248268         0.457440   \n",
       "9       0.395796        0.899978        0.155027         0.542119   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.098299      0.938655  \n",
       "1     0.444234      0.931933  \n",
       "2     0.179584      0.931723  \n",
       "3     0.627599      0.934664  \n",
       "4     0.561437      0.937815  \n",
       "5     0.517958      0.936975  \n",
       "6     0.589792      0.940336  \n",
       "7     0.372401      0.944958  \n",
       "8     0.812854      0.932563  \n",
       "9     0.937500      0.934047  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For class balance\n",
    "\n",
    "svmModel = make_pipeline(StandardScaler(), SVC(C=1.0, kernel='rbf', degree=3 , gamma='auto',class_weight=\"balanced\", random_state=999))\n",
    "\n",
    "scores = cross_validate(svmModel, X, y=y, cv=stratified_cv_object, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "display(pd.DataFrame(scores))\n",
    "\n",
    "scores = cross_validate(svmModel, X, y=y, cv=stratifiedKfold_cv_object, n_jobs=-1, scoring=scoring)\n",
    "display(pd.DataFrame(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"SVMRBF\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Tuning The Model Hyper Parameters for SVM Using Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "param_grid = {\n",
    "     'svc__kernel' : ['poly', 'rbf'],\n",
    "    'svc__C' : np.logspace(-10, 2, 5),\n",
    "    'svc__degree' : [1,2,3],\n",
    "    'svc__gamma': np.logspace(-9, 3, 5)}\n",
    "\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(StandardScaler(), SVC(class_weight='balanced', random_state=999)), \\\n",
    "                   param_grid = param_grid, cv = stratified_cv_object, \\\n",
    "                   verbose=False, n_jobs=-1, scoring=scoring, refit='F1_Score', \\\n",
    "                   return_train_score=True)\n",
    "\n",
    "grid.fit(X, y=y)\n",
    "\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Grid search for finding optimal parameters for this SVM model took long time and we lost the output of the cell during merge process in github. Hence, we are are adding a snapshot of iteration result that we used further to build our better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://github.com/nhendersonkuns/awesomedataminers/raw/master/ReferenceMaterial/RBFGridSearchResults.PNG \"RBF GRid Search Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"SVMPOLY\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Tuning The Model Hyper Parameters for SVM Using Grid Search\n",
    "### For poly kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "param_grid = {\n",
    "     'svc__kernel' : ['poly'],\n",
    "    'svc__C' : [1, 5, 10],\n",
    "    'svc__degree' : [3],\n",
    "    'svc__class_weight' : ['balanced'],\n",
    "    'svc__gamma': [0.01, 1,'auto']}\n",
    "\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "clf = GridSearchCV(make_pipeline(StandardScaler(), SVC(random_state=999)), \\\n",
    "                   param_grid = param_grid, cv = 3, verbose=False, n_jobs=-1, scoring=scoring, refit='AUC', \\\n",
    "                   return_train_score=True)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "       \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    tmpDict = dict()\n",
    "    tmpDict['Accuracy'] = mt.accuracy_score(y_test,y_hat)\n",
    "    tmpDict['AuC'] = mt.roc_auc_score(y_test,y_hat)\n",
    "    tmpDict['F1 Score'] = mt.f1_score(y_test,y_hat)\n",
    "    tmpDict['Precision'] = mt.precision_score(y_test,y_hat)\n",
    "    tmpDict['Recall'] = mt.recall_score(y_test,y_hat)\n",
    "    \n",
    "    modelPerformanceMetrices = modelPerformanceMetrices.append(tmpDict, ignore_index=True)\n",
    "\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "  \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    print(\"Best Estimator Model Parameters\\n\", clf.best_params_)\n",
    "\n",
    "display(modelPerformanceMetrices)\n",
    "print(\"Average Model Performnace Metrices \")\n",
    "display(modelPerformanceMetrices.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"SVMFINAL\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Final SVM Model after GridSearch on Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run SVM model\n",
    "\n",
    "svmModel = make_pipeline(StandardScaler(), SVC(C=1.0, kernel='rbf', degree=3 , gamma=0.01, class_weight = 'balanced', random_state=999))\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    svmModel.fit(X_train, y_train)  # train object\n",
    "    y_hat = svmModel.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy,precision,recall,auc,F1 and confusion matrix for this iterations of training/testing\n",
    "    tmpDict = dict()\n",
    "    tmpDict['Accuracy'] = mt.accuracy_score(y_test,y_hat)\n",
    "    tmpDict['AuC'] = mt.roc_auc_score(y_test,y_hat)\n",
    "    tmpDict['F1 Score'] = mt.f1_score(y_test,y_hat)\n",
    "    tmpDict['Precision'] = mt.precision_score(y_test,y_hat)\n",
    "    tmpDict['Recall'] = mt.recall_score(y_test,y_hat)\n",
    "    \n",
    "    modelPerformanceMetrices = modelPerformanceMetrices.append(tmpDict, ignore_index=True)\n",
    "\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    svm_model = svmModel.named_steps['svc']\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    #print(\"Best Estimator Model Parameters\\n\", svm_model.best_params_)\n",
    "\n",
    "display(modelPerformanceMetrices)\n",
    "print(\"Average Model Performnace Metrices \")\n",
    "display(modelPerformanceMetrices.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **As we can see from above performance metrics this model has performed better as compared to previous model in that the overall average AuC has improved from 0.65 to 0.84. F1 score improved from 0.44 to 0.56. Also, recall has improved significantly from 0.35 to 0.87. That said, overall accuracy decreased a little from 0.90 to 0.84 and precision drop by about 0.20. But since, overall AuC and F1 improved, by changing Prior probabilities, a balance between pricision and recall could be achieved. Moreover, in reality probabilty of people subscribing to a term deposit is not 50-50. Also, increased false positive rate means that bank would be making some extra calls that would not yield positive result but since recall is high, model is less likely to miss people who actually want to subscribe to bank deposit.**\n",
    "\n",
    "- Even though Grid Search provided the optimum value, it took very long time to search the grid and provide optimum value. It took approx 4 hours for one iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"SVMFINAL_Test\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "### Final SVM Model after GridSearch on Additional Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "\n",
    "svmModelAdd = make_pipeline(StandardScaler(), SVC(C=1.0, kernel='rbf', degree=3 , gamma=0.01, class_weight = 'balanced', random_state=999))\n",
    "\n",
    "# Fit the whole training dataset now, since validation would be done on additional dataset\n",
    "svmModelAdd.fit(X,y) \n",
    "y_hat = svmModelAdd.predict(X_Final) # get test set precitions\n",
    "\n",
    "# now let's get the accuracy,precision,recall,auc,F1 and confusion matrix for this iterations of training/testing\n",
    "tmpDict = dict()\n",
    "tmpDict['Accuracy'] = mt.accuracy_score(y_Final,y_hat)\n",
    "tmpDict['AuC'] = mt.roc_auc_score(y_Final,y_hat)\n",
    "tmpDict['F1 Score'] = mt.f1_score(y_Final,y_hat)\n",
    "tmpDict['Precision'] = mt.precision_score(y_Final,y_hat)\n",
    "tmpDict['Recall'] = mt.recall_score(y_Final,y_hat)\n",
    "    \n",
    "modelPerformanceMetrices = modelPerformanceMetrices.append(tmpDict, ignore_index=True)\n",
    "\n",
    "conf = mt.confusion_matrix(y_Final,y_hat)\n",
    "  \n",
    "print(\"confusion matrix\\n\",conf)\n",
    "    \n",
    "print(\"\\n Model Performnace Metrices\")\n",
    "display(modelPerformanceMetrices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, the results obtained for classification performance metrices(AuC, accuracy, F1 score etc) on the additional test dataset are very close to those obtained on cross-validation dataset. And final model is performing significantly better(in terms of overall AuC values) than the initial svm model the one without hyperparameters tuning.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"MODELADV\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Model Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SVM, finding the optimum hyper parameters took very long time. For poly and rbf kernels, it took hours to find the best parameter. \n",
    "\n",
    "Once the best parameters were found for each kernels, then individual models ran fast.\n",
    "\n",
    "Both Support Vector Machine (SVM) and Logistic Regression (LR) try to classify a binary response or maximizing the probability of classifying a response variable.  In these models the team is trying to classify a response variable of subscribing to a long-term deposit.\n",
    "\n",
    "The SVM model attempts to find the maximum margin in the dimensional space.  This space is defined by the number of features that classify data points.  Points that fall on either side of this plane are classified into one of the binary responses the model is predicting.  To accurately define these margins the model uses support vectors that define the boundaries or the max margin.  These support vectors are points that lie the closest to the calculated boundary of the points.  These support vectors also define the position of the optimized plane.\n",
    "\n",
    "![alt text](https://github.com/nhendersonkuns/awesomedataminers/raw/master/ReferenceMaterial/SVM_optimal_plane.PNG \"SVM Planes\")\n",
    "\n",
    "The LR model will take the output of you model and give you a response that is between 0 and 1 using the logistic function.  In the case that the output is higher or lower than your threshold the value will be give a 1 or 0 respectively.  Within LR the method is to maximize the likelihood that a random data point is classified correctly (maximum likelihood estimation, MLE).  The LR allows the model to optimize this function by use of algorithms such as Newtons method, conjugate gradients, modifications of Newtons method using box constraints.  Learning and applying the different methods allow for a more accurate model.\n",
    "\n",
    "The difference between SVM and LR in terms of the Loss function is that SVM will minimize hinge loss while LR minimizes logistic loss.  What this leads to is logistic loss (LR) diverges faster than hinge loss (SVM).  The LR model is more sensitive to outliers as it tries to find the plane for classifying the points, where as the SVM model is not as sensitive to these outliers.\n",
    "\n",
    "![alt text](https://github.com/nhendersonkuns/awesomedataminers/raw/master/ReferenceMaterial/Outliers_SVM_vs_LR.PNG \"SVM vs LR Outliers\")\n",
    "\n",
    "Within the LR model your values are predicted probabilities between 0 and 1, which at this point you must decide your cut-off to give you the binary response.  The SVM model does produce a final binary response of 0 or 1.\n",
    "\n",
    "LR will perform better on smaller data sets and SVM performs better with larger data sets.  In terms of small data sets SVM as support vectors may not be a true/good representation of the decision boundaries.  Deciding the performance between SVM and LR, is relative and depends on several factors such as data set size and domain knowledge of the data.  In most cases a simple model (LR) should be attempted first to determine if the output is of desired accuracy.  In tradition LR it is found that the models may fit the training data set to well and may result in a model that is unable to make reasonably good predictions for unknown data points which is referred to as overfitting.  SVM attempts to minimize the classification error on the training set and minimize the complexity of the model.  Over all between the two methods SVM can compute more complex decision boundaries.\n",
    "\n",
    "**In the case of the bank data set, the team believes that the SVM should provide better results due to the size of the data sets and several outlier points that were discovered during data discovery.   Ultimately, the SVM model performed just as well as the logistic model in our case.  However, the major difference between the two models was the time for the SVM to run and the SVM model's benefit did not outweigh the time that was spent for it to run.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"INTVECT\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Interpret Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svmModel.named_steps['svc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_model.support_vectors_.shape)\n",
    "print(svm_model.support_.shape)\n",
    "print(svm_model.n_support_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are in total 13577 support vectors.\n",
    "\n",
    "The total number of different support vectors are 13577.The total number postive support vecotrs for those that subscribed are \n",
    "11695.The total number of negative support vectors for those that did not subscribe are 1822."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = bankPromoModel_Df.iloc[train_indices] # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_model.support_,:]\n",
    "\n",
    "df_support['Target'] = y[svm_model.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "bankPromoModel_Df['Target'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "#from pandas.tools.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['Target'])\n",
    "df_grouped = bankPromoModel_Df.groupby(['Target'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['age', 'balance', 'duration','previous']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Not Subscribed','Subscribed'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Not Subscribed','Subscribed'])\n",
    "    plt.title(v+' (Original)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visual examination of the plots, the original data and support vectors look very close. Actually the separation is lot greater in the original data than what is shown in the plots. The reason is that the support vector instances are the data points that are only on the edge of the class boundary and classified incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________\n",
    "<a id=\"ECPWORK\"></a>\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "# Exceptional Work\n",
    "## Running Polynomial Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "logisticModel = make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), LogisticRegression(penalty='l2', C=1.0, class_weight=None, random_state=999))\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "       \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    logisticModel.fit(X_train,y_train)  # train object\n",
    "    y_hat = logisticModel.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy,precision,recall,auc,F1 and confusion matrix for this iterations of training/testing\n",
    "    tmpDict = dict()\n",
    "    tmpDict['Accuracy'] = mt.accuracy_score(y_test,y_hat)\n",
    "    tmpDict['AuC'] = mt.roc_auc_score(y_test,y_hat)\n",
    "    tmpDict['F1 Score'] = mt.f1_score(y_test,y_hat)\n",
    "    tmpDict['Precision'] = mt.precision_score(y_test,y_hat)\n",
    "    tmpDict['Recall'] = mt.recall_score(y_test,y_hat)\n",
    "    \n",
    "    modelPerformanceMetrices = modelPerformanceMetrices.append(tmpDict, ignore_index=True)\n",
    "\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "  \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "\n",
    "display(modelPerformanceMetrices)\n",
    "print(\"Average Model Performnace Metrices \")\n",
    "display(modelPerformanceMetrices.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from output above, we see that polynomial model(having higher complexity than earlier model) is having much better AuC, Precision and recall scores and at higher accuracy, even without hyper parameters tuning. This points to the fact that our earlier model was underfitting and we need to add complexity to it to achieve better results.\n",
    "Next we would perform grid search to tune hyper parameters of this polynomial logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "     'logisticregression__penalty' : ['l1', 'l2'],\n",
    "    'logisticregression__C' : np.logspace(-4, 4, 50),\n",
    "    'logisticregression__solver' : ['liblinear'],\n",
    "    'logisticregression__class_weight' : [None, 'balanced']}\n",
    "\n",
    "\n",
    "# Create grid search object\n",
    "# Trying to find params that lead to maximum F1 Score\n",
    "clf = GridSearchCV(make_pipeline(StandardScaler(), PolynomialFeatures(degree=2), LogisticRegression(random_state=999)), \\\n",
    "                   param_grid = param_grid, cv = 5, verbose=False, n_jobs=-1, scoring='f1')\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "       \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    tmpDict = dict()\n",
    "    tmpDict['Accuracy'] = mt.accuracy_score(y_test,y_hat)\n",
    "    tmpDict['AuC'] = mt.roc_auc_score(y_test,y_hat)\n",
    "    tmpDict['F1 Score'] = mt.f1_score(y_test,y_hat)\n",
    "    tmpDict['Precision'] = mt.precision_score(y_test,y_hat)\n",
    "    tmpDict['Recall'] = mt.recall_score(y_test,y_hat)\n",
    "    \n",
    "    modelPerformanceMetrices = modelPerformanceMetrices.append(tmpDict, ignore_index=True)\n",
    "\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "  \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    print(\"Best Estimator Model Parameters\\n\", clf.best_params_)\n",
    "\n",
    "display(modelPerformanceMetrices)\n",
    "print(\"Average Model Performnace Metrices \")\n",
    "display(modelPerformanceMetrices.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Recursive Feature Elimination to Reduce Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the RFE in CV mode, in 5 Fold CV there would be 5 passes and in each pass features that are not that important to classification task are listed as output.\n",
    "Remove a feature from the dataset, if the feature is marked for elimination by RFE in 2 passes out of 5.\n",
    "Recreate the dataset with only non-eliminated features and fit the model again on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "featureSelectionArr = np.full((len(bankPromoModel_Df.columns)), True)\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "scl_obj = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None)\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # Standardize the train dataset and then apply the transform the test data. \n",
    "    scl_obj.fit(X_train)\n",
    "    X_train_scaled = scl_obj.transform(X_train)\n",
    "    X_test_scaled = scl_obj.transform(X_test)\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "#    lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "    rfe = RFE(lr_clf, 25)\n",
    "    fit = rfe.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "    featureSelectionArr = np.vstack((featureSelectionArr, fit.support_ ))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first row in feature array as it was dummy row.\n",
    "featureSelectionArr = featureSelectionArr[1: , :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only those feature that appear important in 3 out of 5 passes.\n",
    "bankPromoModel_Df.columns[np.sum(featureSelectionArr, axis = 0) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of original data frame\n",
    "bankPromoModel_Df_1 = bankPromo_df.copy()\n",
    "bankPromoModel_Df_1['Target'] = bankPromoModel_Df_1['Subscribed'].apply(lambda resp : 1 if resp == \"yes\" else 0)\n",
    "bankPromoModel_Df_1['Target'] = bankPromoModel_Df_1['Target'].astype(np.int)\n",
    "# Delete the original 'Subscribed' column\n",
    "del bankPromoModel_Df_1['Subscribed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert all categorical variables to corresponding indicator variables\n",
    "categoricalVarsToBeUsed = ['job',\n",
    " 'marital',\n",
    " 'education',\n",
    " 'housing',\n",
    " 'loan',\n",
    " 'contact',\n",
    " 'month',\n",
    " 'poutcome']\n",
    "\n",
    "for categoricalVar in categoricalVarsToBeUsed:\n",
    "    tmpDf = pd.DataFrame()\n",
    "    # Remove 1st class level to avoid multicollinearity\n",
    "    tmpDf = pd.get_dummies(bankPromoModel_Df_1[categoricalVar], prefix=categoricalVar, drop_first=True)\n",
    "    bankPromoModel_Df_1 = pd.concat((bankPromoModel_Df_1, tmpDf), axis=1)\n",
    "\n",
    "# Now remove the original categorical vars since indicator variables are created from them.\n",
    "bankPromoModel_Df_1.drop(categoricalVars, inplace=True, axis=1)\n",
    "\n",
    "# Now remove non useful numerical variables\n",
    "bankPromoModel_Df_1.drop(['age', 'balance', 'pdays', 'previous'], inplace=True, axis=1)\n",
    "bankPromoModel_Df_1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "if 'Target' in bankPromoModel_Df_1:\n",
    "    y = bankPromoModel_Df_1['Target'].values # get the labels we want\n",
    "    del bankPromoModel_Df_1['Target']        # get rid of the class label\n",
    "    X = bankPromoModel_Df_1.values           # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# To use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 5\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the model on reduced dataset\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "degrees = np.arange(1, 2)\n",
    "\n",
    "model = make_pipeline(StandardScaler(),LogisticRegression(penalty='l2', C=1.0, class_weight=None))\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "       \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    model.fit(X_train,y_train)  # train object\n",
    "    y_hat = model.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    pre = mt.precision_score(y_test,y_hat)\n",
    "    rec = mt.recall_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    auc = mt.roc_auc_score(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"AuC\", auc )\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"precision\", pre)\n",
    "    print(\"recall\", rec)\n",
    "    print(\"confusion matrix\\n\",conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above output that even after performing recursive feature elimination(RFE), our model performance metrics are not very different from the that of initial simple logistic regression model. Hence, we would be keeping all features in model and rely upon regularization to prevent our model from overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
