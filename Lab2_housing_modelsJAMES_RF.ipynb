{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Dataset Ready for Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime \n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "# To display plots inside the iPython Notebook itself\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"age\";\"job\";\"marital\";\"education\";\"default\";\"balance\";\"housing\";\"loan\";\"contact\";\"day\";\"month\";\"duration\";\"campaign\";\"pdays\";\"previous\";\"poutcome\";\"y\"\n",
      "\n",
      "58;\"management\";\"married\";\"tertiary\";\"no\";2143;\"yes\";\"no\";\"unknown\";5;\"may\";261;1;-1;0;\"unknown\";\"no\"\n",
      "\n",
      "44;\"technician\";\"single\";\"secondary\";\"no\";29;\"yes\";\"no\";\"unknown\";5;\"may\";151;1;-1;0;\"unknown\";\"no\"\n",
      "\n",
      "33;\"entrepreneur\";\"married\";\"secondary\";\"no\";2;\"yes\";\"yes\";\"unknown\";5;\"may\";76;1;-1;0;\"unknown\";\"no\"\n",
      "\n",
      "47;\"blue-collar\";\"married\";\"unknown\";\"no\";1506;\"yes\";\"no\";\"unknown\";5;\"may\";92;1;-1;0;\"unknown\";\"no\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>Subscribed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>blue-collar</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1506</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>unknown</td>\n",
       "      <td>single</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>231</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>139</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>28</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>447</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>unknown</td>\n",
       "      <td>5</td>\n",
       "      <td>may</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
       "4   33       unknown   single    unknown      no        1      no   no   \n",
       "5   35    management  married   tertiary      no      231     yes   no   \n",
       "6   28    management   single   tertiary      no      447     yes  yes   \n",
       "\n",
       "   contact  day month  duration  campaign  pdays  previous poutcome Subscribed  \n",
       "0  unknown    5   may       261         1     -1         0  unknown         no  \n",
       "1  unknown    5   may       151         1     -1         0  unknown         no  \n",
       "2  unknown    5   may        76         1     -1         0  unknown         no  \n",
       "3  unknown    5   may        92         1     -1         0  unknown         no  \n",
       "4  unknown    5   may       198         1     -1         0  unknown         no  \n",
       "5  unknown    5   may       139         1     -1         0  unknown         no  \n",
       "6  unknown    5   may       217         1     -1         0  unknown         no  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To verify how data is orgainzed in file(to find the delimiter) and then\n",
    "# use corresponding function to open the file. eg\n",
    "# data could be in .csv. .tsv, excel format etc.\n",
    "pathOfDataFile = \"data/bank-full.csv\"\n",
    "firstFewLines = list()\n",
    "noOfLinesToView = 5\n",
    "\n",
    "with open(pathOfDataFile) as dataFile:\n",
    "    firstFewLines = [next(dataFile) for i in range(noOfLinesToView)]\n",
    "    for line in firstFewLines:\n",
    "        print(line)\n",
    "\n",
    "# Import the semi-colon delimited data file into pandas dataFrame\n",
    "bankPromo_df = pd.read_csv(pathOfDataFile, sep = \";\")\n",
    "\n",
    "# Rename the Target/Final Outcome column from \"y\" to \"Subscribed\" as based on data description.\n",
    "bankPromo_df = bankPromo_df.rename(columns={\"y\":\"Subscribed\"})\n",
    "\n",
    "bankPromo_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      "age           45211 non-null int64\n",
      "job           45211 non-null object\n",
      "marital       45211 non-null object\n",
      "education     45211 non-null object\n",
      "default       45211 non-null object\n",
      "balance       45211 non-null int64\n",
      "loan          45211 non-null object\n",
      "contact       45211 non-null object\n",
      "day           45211 non-null int64\n",
      "month         45211 non-null object\n",
      "duration      45211 non-null int64\n",
      "campaign      45211 non-null int64\n",
      "pdays         45211 non-null int64\n",
      "previous      45211 non-null int64\n",
      "poutcome      45211 non-null object\n",
      "Subscribed    45211 non-null object\n",
      "Target        45211 non-null int32\n",
      "dtypes: int32(1), int64(7), object(9)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of original data frame\n",
    "bankPromoModel_hsng_Df = bankPromo_df.copy()\n",
    "bankPromoModel_hsng_Df['Target'] = bankPromoModel_hsng_Df['housing'].apply(lambda resp : 1 if resp == \"yes\" else 0)\n",
    "bankPromoModel_hsng_Df['Target'] = bankPromoModel_hsng_Df['Target'].astype(np.int)\n",
    "\n",
    "# Delete the original 'housing' column\n",
    "del bankPromoModel_hsng_Df['housing']\n",
    "\n",
    "# List final variables of the new dataset\n",
    "bankPromoModel_hsng_Df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of categorical variables , keeping 'housing' as target/response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "['job', 'marital', 'education', 'default', 'loan', 'contact', 'month', 'poutcome', 'Subscribed']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique values(Levels) for categorical variables.\n",
    "# List to hold names of categorical variables\n",
    "categoricalVars = list()\n",
    "# List to hold names of numerical variables\n",
    "numericalVars = list()\n",
    "\n",
    "for colName in bankPromo_df.columns:\n",
    "    if bankPromo_df[colName].dtype == np.int64:\n",
    "        numericalVars.append(colName)\n",
    "    elif bankPromo_df[colName].dtype == np.object:\n",
    "        categoricalVars.append(colName)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Remove Target column from final categorical Var list\n",
    "categoricalVars.remove('housing')\n",
    "\n",
    "print(numericalVars)\n",
    "print(categoricalVars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 17 columns):\n",
      "age           4521 non-null int64\n",
      "job           4521 non-null object\n",
      "marital       4521 non-null object\n",
      "education     4521 non-null object\n",
      "default       4521 non-null object\n",
      "balance       4521 non-null int64\n",
      "loan          4521 non-null object\n",
      "contact       4521 non-null object\n",
      "day           4521 non-null int64\n",
      "month         4521 non-null object\n",
      "duration      4521 non-null int64\n",
      "campaign      4521 non-null int64\n",
      "pdays         4521 non-null int64\n",
      "previous      4521 non-null int64\n",
      "poutcome      4521 non-null object\n",
      "Subscribed    4521 non-null object\n",
      "Target        4521 non-null int32\n",
      "dtypes: int32(1), int64(7), object(9)\n",
      "memory usage: 582.9+ KB\n"
     ]
    }
   ],
   "source": [
    "pathOfAdditionalDataFile = \"data/bank.csv\"\n",
    "\n",
    "# Import the semi-colon delimited data file into pandas dataFrame\n",
    "bankPromoAdditional_h_df = pd.read_csv(pathOfAdditionalDataFile, sep = \";\")\n",
    "\n",
    "# Rename the Target/Final Outcome column from \"y\" to \"Subscribed\" as based on data description.\n",
    "bankPromoAdditional_h_df = bankPromoAdditional_h_df.rename(columns={\"y\":\"Subscribed\"})\n",
    "\n",
    "bankPromoAdditional_h_df['Target'] = bankPromoAdditional_h_df['housing'].apply(lambda resp : 1 if resp == \"yes\" else 0)\n",
    "bankPromoAdditional_h_df['Target'] = bankPromoAdditional_h_df['Target'].astype(np.int)\n",
    "# Delete the original 'housing' column\n",
    "del bankPromoAdditional_h_df['housing']\n",
    "\n",
    "bankPromoAdditional_h_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Delete any features that do not relate to the response variable in the business sense\n",
    "\n",
    "BankPromo dataset contains \n",
    "\n",
    "i) bank client data like age, balance, education, job , marital status , any loans - housing/personal , if they have defaulted  \n",
    "\n",
    "ii) information regarding last contact  in current campaign - contact type, day/month when last contacted, duration of last call \n",
    "\n",
    "iii) other attributes like number of times contacted during current campaign ,number of days since last contact, number of contacts made in last campaign, outcome from previous campaign , outcome from current campagin i.e. Subscribed or not  \n",
    "\n",
    "\n",
    "From business use case perspective, any information regarding Term deposit subscription campaign may have no effect on the client data but vice-versa may not be true. It will be interesting to find what features from the campaign information should be considered to predict if a customer has a housing loan or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform One Hot Encoding for categorical variables in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset info\n",
      "=====================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 43 columns):\n",
      "age                    45211 non-null int64\n",
      "balance                45211 non-null int64\n",
      "day                    45211 non-null int64\n",
      "duration               45211 non-null int64\n",
      "campaign               45211 non-null int64\n",
      "pdays                  45211 non-null int64\n",
      "previous               45211 non-null int64\n",
      "Target                 45211 non-null int32\n",
      "job_blue-collar        45211 non-null uint8\n",
      "job_entrepreneur       45211 non-null uint8\n",
      "job_housemaid          45211 non-null uint8\n",
      "job_management         45211 non-null uint8\n",
      "job_retired            45211 non-null uint8\n",
      "job_self-employed      45211 non-null uint8\n",
      "job_services           45211 non-null uint8\n",
      "job_student            45211 non-null uint8\n",
      "job_technician         45211 non-null uint8\n",
      "job_unemployed         45211 non-null uint8\n",
      "job_unknown            45211 non-null uint8\n",
      "marital_married        45211 non-null uint8\n",
      "marital_single         45211 non-null uint8\n",
      "education_secondary    45211 non-null uint8\n",
      "education_tertiary     45211 non-null uint8\n",
      "education_unknown      45211 non-null uint8\n",
      "default_yes            45211 non-null uint8\n",
      "loan_yes               45211 non-null uint8\n",
      "contact_telephone      45211 non-null uint8\n",
      "contact_unknown        45211 non-null uint8\n",
      "month_aug              45211 non-null uint8\n",
      "month_dec              45211 non-null uint8\n",
      "month_feb              45211 non-null uint8\n",
      "month_jan              45211 non-null uint8\n",
      "month_jul              45211 non-null uint8\n",
      "month_jun              45211 non-null uint8\n",
      "month_mar              45211 non-null uint8\n",
      "month_may              45211 non-null uint8\n",
      "month_nov              45211 non-null uint8\n",
      "month_oct              45211 non-null uint8\n",
      "month_sep              45211 non-null uint8\n",
      "poutcome_other         45211 non-null uint8\n",
      "poutcome_success       45211 non-null uint8\n",
      "poutcome_unknown       45211 non-null uint8\n",
      "Subscribed_yes         45211 non-null uint8\n",
      "dtypes: int32(1), int64(7), uint8(35)\n",
      "memory usage: 4.1 MB\n",
      "Test dataset info\n",
      "=====================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4521 entries, 0 to 4520\n",
      "Data columns (total 42 columns):\n",
      "age                    4521 non-null int64\n",
      "balance                4521 non-null int64\n",
      "day                    4521 non-null int64\n",
      "duration               4521 non-null int64\n",
      "campaign               4521 non-null int64\n",
      "pdays                  4521 non-null int64\n",
      "previous               4521 non-null int64\n",
      "job_blue-collar        4521 non-null uint8\n",
      "job_entrepreneur       4521 non-null uint8\n",
      "job_housemaid          4521 non-null uint8\n",
      "job_management         4521 non-null uint8\n",
      "job_retired            4521 non-null uint8\n",
      "job_self-employed      4521 non-null uint8\n",
      "job_services           4521 non-null uint8\n",
      "job_student            4521 non-null uint8\n",
      "job_technician         4521 non-null uint8\n",
      "job_unemployed         4521 non-null uint8\n",
      "job_unknown            4521 non-null uint8\n",
      "marital_married        4521 non-null uint8\n",
      "marital_single         4521 non-null uint8\n",
      "education_secondary    4521 non-null uint8\n",
      "education_tertiary     4521 non-null uint8\n",
      "education_unknown      4521 non-null uint8\n",
      "default_yes            4521 non-null uint8\n",
      "loan_yes               4521 non-null uint8\n",
      "contact_telephone      4521 non-null uint8\n",
      "contact_unknown        4521 non-null uint8\n",
      "month_aug              4521 non-null uint8\n",
      "month_dec              4521 non-null uint8\n",
      "month_feb              4521 non-null uint8\n",
      "month_jan              4521 non-null uint8\n",
      "month_jul              4521 non-null uint8\n",
      "month_jun              4521 non-null uint8\n",
      "month_mar              4521 non-null uint8\n",
      "month_may              4521 non-null uint8\n",
      "month_nov              4521 non-null uint8\n",
      "month_oct              4521 non-null uint8\n",
      "month_sep              4521 non-null uint8\n",
      "poutcome_other         4521 non-null uint8\n",
      "poutcome_success       4521 non-null uint8\n",
      "poutcome_unknown       4521 non-null uint8\n",
      "Subscribed_yes         4521 non-null uint8\n",
      "dtypes: int64(7), uint8(35)\n",
      "memory usage: 401.8 KB\n"
     ]
    }
   ],
   "source": [
    "## Training Dataset\n",
    "###################\n",
    "# Convert all categorical variables to corresponding indicator variables\n",
    "for categoricalVar in categoricalVars:\n",
    "    tmpDf = pd.DataFrame()\n",
    "    # Remove 1st class level to avoid multicollinearity\n",
    "    tmpDf = pd.get_dummies(bankPromoModel_hsng_Df[categoricalVar], prefix=categoricalVar, drop_first=True)\n",
    "    bankPromoModel_hsng_Df = pd.concat((bankPromoModel_hsng_Df, tmpDf), axis=1)\n",
    "\n",
    "# Now remove the original categorical vars since indicator variables are created from them.\n",
    "bankPromoModel_hsng_Df.drop(categoricalVars, inplace=True, axis=1)\n",
    "print(\"Training dataset info\")\n",
    "print(\"=====================\")\n",
    "bankPromoModel_hsng_Df.info()\n",
    "\n",
    "\n",
    "## Test Dataset\n",
    "###################\n",
    "# Covert all categorical variables to corresponding indicator variables\n",
    "for categoricalVar in categoricalVars:\n",
    "    tmpDf = pd.DataFrame()\n",
    "    # Remove 1st class level to avoid multicollinearity\n",
    "    tmpDf = pd.get_dummies(bankPromoAdditional_h_df[categoricalVar], prefix=categoricalVar, drop_first=True)\n",
    "    bankPromoAdditional_h_df = pd.concat((bankPromoAdditional_h_df, tmpDf), axis=1)\n",
    "\n",
    "# Now remove the original categorical vars since indicator variables are created from them.\n",
    "bankPromoAdditional_h_df.drop(categoricalVars, inplace=True, axis=1)\n",
    "\n",
    "if 'Target' in bankPromoAdditional_h_df:\n",
    "    y_Final = bankPromoAdditional_h_df['Target'].values # get the labels we want\n",
    "    del bankPromoAdditional_h_df['Target']        # get rid of the class label\n",
    "    X_Final = bankPromoAdditional_h_df.values\n",
    "\n",
    "print(\"Test dataset info\")\n",
    "print(\"=====================\")\n",
    "bankPromoAdditional_h_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 10 Splits  Cross Validation Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=10, random_state=999, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "# Training and Test Split\n",
    "# Since housing is a balanced dataset ( with 56% yes and 44% No , we will use simple KFold and ShuffleSplit cv objects)\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if 'Target' in bankPromoModel_hsng_Df:\n",
    "    y = bankPromoModel_hsng_Df['Target'].values # get the labels we want\n",
    "    del bankPromoModel_hsng_Df['Target']        # get rid of the class label\n",
    "    X = bankPromoModel_hsng_Df.values           # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# To use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 10\n",
    "kfold_cv_object = KFold(n_splits=num_cv_iterations , shuffle = False , random_state=999)\n",
    "#n_splits=num_cv_iterations,test_size  = 0.2, random_state=999\n",
    "                         \n",
    "print(kfold_cv_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=10, random_state=999, test_size=0.1, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "# To use the cross validation object in scikit learn, we need to grab an instance\n",
    "# of the object and set it up. This object will be able to split our data into \n",
    "# training and testing splits\n",
    "num_cv_iterations = 10\n",
    "shuffle_cv_object = ShuffleSplit(n_splits=num_cv_iterations, test_size = 0.1, random_state=999)\n",
    "                         \n",
    "print(shuffle_cv_object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Logistic Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.043045</td>\n",
       "      <td>0.114479</td>\n",
       "      <td>0.944613</td>\n",
       "      <td>0.744731</td>\n",
       "      <td>0.574598</td>\n",
       "      <td>0.816525</td>\n",
       "      <td>0.895179</td>\n",
       "      <td>0.735653</td>\n",
       "      <td>0.897026</td>\n",
       "      <td>0.745085</td>\n",
       "      <td>0.997532</td>\n",
       "      <td>0.744378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.879607</td>\n",
       "      <td>0.109446</td>\n",
       "      <td>0.853853</td>\n",
       "      <td>0.751690</td>\n",
       "      <td>0.585186</td>\n",
       "      <td>0.824778</td>\n",
       "      <td>0.752931</td>\n",
       "      <td>0.745441</td>\n",
       "      <td>0.908155</td>\n",
       "      <td>0.759814</td>\n",
       "      <td>0.805679</td>\n",
       "      <td>0.743738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.886063</td>\n",
       "      <td>0.085486</td>\n",
       "      <td>0.500095</td>\n",
       "      <td>0.809572</td>\n",
       "      <td>0.627757</td>\n",
       "      <td>0.842206</td>\n",
       "      <td>0.418491</td>\n",
       "      <td>0.771369</td>\n",
       "      <td>0.343252</td>\n",
       "      <td>0.786251</td>\n",
       "      <td>0.920868</td>\n",
       "      <td>0.834318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.041555</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>0.088985</td>\n",
       "      <td>0.800586</td>\n",
       "      <td>0.538851</td>\n",
       "      <td>0.849564</td>\n",
       "      <td>0.438399</td>\n",
       "      <td>0.779258</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.802225</td>\n",
       "      <td>0.048381</td>\n",
       "      <td>0.798954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.826891</td>\n",
       "      <td>0.100516</td>\n",
       "      <td>0.245696</td>\n",
       "      <td>0.792306</td>\n",
       "      <td>0.669041</td>\n",
       "      <td>0.821918</td>\n",
       "      <td>0.786773</td>\n",
       "      <td>0.748169</td>\n",
       "      <td>0.581481</td>\n",
       "      <td>0.775134</td>\n",
       "      <td>0.155754</td>\n",
       "      <td>0.810256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.825351</td>\n",
       "      <td>0.095947</td>\n",
       "      <td>0.590136</td>\n",
       "      <td>0.786064</td>\n",
       "      <td>0.761794</td>\n",
       "      <td>0.834171</td>\n",
       "      <td>0.685689</td>\n",
       "      <td>0.757483</td>\n",
       "      <td>0.719409</td>\n",
       "      <td>0.786815</td>\n",
       "      <td>0.500244</td>\n",
       "      <td>0.785315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.992232</td>\n",
       "      <td>0.097947</td>\n",
       "      <td>0.455639</td>\n",
       "      <td>0.792442</td>\n",
       "      <td>0.696512</td>\n",
       "      <td>0.838221</td>\n",
       "      <td>0.629507</td>\n",
       "      <td>0.761637</td>\n",
       "      <td>0.705231</td>\n",
       "      <td>0.781817</td>\n",
       "      <td>0.336534</td>\n",
       "      <td>0.803358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.823812</td>\n",
       "      <td>0.080319</td>\n",
       "      <td>0.878076</td>\n",
       "      <td>0.754189</td>\n",
       "      <td>0.772711</td>\n",
       "      <td>0.822239</td>\n",
       "      <td>0.807122</td>\n",
       "      <td>0.742910</td>\n",
       "      <td>0.891286</td>\n",
       "      <td>0.762158</td>\n",
       "      <td>0.865252</td>\n",
       "      <td>0.746384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.818219</td>\n",
       "      <td>0.082956</td>\n",
       "      <td>0.900643</td>\n",
       "      <td>0.760999</td>\n",
       "      <td>0.843792</td>\n",
       "      <td>0.821425</td>\n",
       "      <td>0.846273</td>\n",
       "      <td>0.741927</td>\n",
       "      <td>0.857376</td>\n",
       "      <td>0.755513</td>\n",
       "      <td>0.948509</td>\n",
       "      <td>0.766564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.037698</td>\n",
       "      <td>0.077663</td>\n",
       "      <td>0.377330</td>\n",
       "      <td>0.800729</td>\n",
       "      <td>0.646786</td>\n",
       "      <td>0.829804</td>\n",
       "      <td>0.630613</td>\n",
       "      <td>0.758270</td>\n",
       "      <td>0.292317</td>\n",
       "      <td>0.784798</td>\n",
       "      <td>0.532072</td>\n",
       "      <td>0.817321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  2.043045    0.114479       0.944613        0.744731  0.574598   0.816525   \n",
       "1  1.879607    0.109446       0.853853        0.751690  0.585186   0.824778   \n",
       "2  1.886063    0.085486       0.500095        0.809572  0.627757   0.842206   \n",
       "3  2.041555    0.093085       0.088985        0.800586  0.538851   0.849564   \n",
       "4  1.826891    0.100516       0.245696        0.792306  0.669041   0.821918   \n",
       "5  1.825351    0.095947       0.590136        0.786064  0.761794   0.834171   \n",
       "6  1.992232    0.097947       0.455639        0.792442  0.696512   0.838221   \n",
       "7  3.823812    0.080319       0.878076        0.754189  0.772711   0.822239   \n",
       "8  1.818219    0.082956       0.900643        0.760999  0.843792   0.821425   \n",
       "9  2.037698    0.077663       0.377330        0.800729  0.646786   0.829804   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.895179        0.735653        0.897026         0.745085   \n",
       "1       0.752931        0.745441        0.908155         0.759814   \n",
       "2       0.418491        0.771369        0.343252         0.786251   \n",
       "3       0.438399        0.779258        0.553571         0.802225   \n",
       "4       0.786773        0.748169        0.581481         0.775134   \n",
       "5       0.685689        0.757483        0.719409         0.786815   \n",
       "6       0.629507        0.761637        0.705231         0.781817   \n",
       "7       0.807122        0.742910        0.891286         0.762158   \n",
       "8       0.846273        0.741927        0.857376         0.755513   \n",
       "9       0.630613        0.758270        0.292317         0.784798   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.997532      0.744378  \n",
       "1     0.805679      0.743738  \n",
       "2     0.920868      0.834318  \n",
       "3     0.048381      0.798954  \n",
       "4     0.155754      0.810256  \n",
       "5     0.500244      0.785315  \n",
       "6     0.336534      0.803358  \n",
       "7     0.865252      0.746384  \n",
       "8     0.948509      0.766564  \n",
       "9     0.532072      0.817321  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           2.117447\n",
       "score_time         0.093784\n",
       "test_F1_Score      0.583507\n",
       "train_F1_Score     0.779331\n",
       "test_AUC           0.671703\n",
       "train_AUC          0.830085\n",
       "test_Accuracy      0.689098\n",
       "train_Accuracy     0.754212\n",
       "test_Precision     0.674911\n",
       "train_Precision    0.773961\n",
       "test_Recall        0.611083\n",
       "train_Recall       0.785059\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Logistic Regression with Kfold cv object\n",
      "0:00:11.708129\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.184534</td>\n",
       "      <td>0.077950</td>\n",
       "      <td>0.775268</td>\n",
       "      <td>0.778855</td>\n",
       "      <td>0.827496</td>\n",
       "      <td>0.829789</td>\n",
       "      <td>0.749226</td>\n",
       "      <td>0.752316</td>\n",
       "      <td>0.772817</td>\n",
       "      <td>0.773054</td>\n",
       "      <td>0.777734</td>\n",
       "      <td>0.784745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.150011</td>\n",
       "      <td>0.124972</td>\n",
       "      <td>0.788132</td>\n",
       "      <td>0.777900</td>\n",
       "      <td>0.835867</td>\n",
       "      <td>0.828970</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.750940</td>\n",
       "      <td>0.783452</td>\n",
       "      <td>0.771810</td>\n",
       "      <td>0.792869</td>\n",
       "      <td>0.784086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.131026</td>\n",
       "      <td>0.095480</td>\n",
       "      <td>0.781732</td>\n",
       "      <td>0.778234</td>\n",
       "      <td>0.838677</td>\n",
       "      <td>0.828603</td>\n",
       "      <td>0.755860</td>\n",
       "      <td>0.751505</td>\n",
       "      <td>0.779882</td>\n",
       "      <td>0.771818</td>\n",
       "      <td>0.783591</td>\n",
       "      <td>0.784757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.261994</td>\n",
       "      <td>0.120514</td>\n",
       "      <td>0.782011</td>\n",
       "      <td>0.778980</td>\n",
       "      <td>0.833092</td>\n",
       "      <td>0.829253</td>\n",
       "      <td>0.754533</td>\n",
       "      <td>0.751923</td>\n",
       "      <td>0.767836</td>\n",
       "      <td>0.772082</td>\n",
       "      <td>0.796719</td>\n",
       "      <td>0.786002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.388806</td>\n",
       "      <td>0.178146</td>\n",
       "      <td>0.787298</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.838794</td>\n",
       "      <td>0.828522</td>\n",
       "      <td>0.764485</td>\n",
       "      <td>0.750399</td>\n",
       "      <td>0.781833</td>\n",
       "      <td>0.770795</td>\n",
       "      <td>0.792840</td>\n",
       "      <td>0.784888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.307807</td>\n",
       "      <td>0.125958</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>0.777182</td>\n",
       "      <td>0.831703</td>\n",
       "      <td>0.829384</td>\n",
       "      <td>0.756966</td>\n",
       "      <td>0.750694</td>\n",
       "      <td>0.785601</td>\n",
       "      <td>0.771488</td>\n",
       "      <td>0.779093</td>\n",
       "      <td>0.782961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.366782</td>\n",
       "      <td>0.121443</td>\n",
       "      <td>0.774514</td>\n",
       "      <td>0.778807</td>\n",
       "      <td>0.824350</td>\n",
       "      <td>0.830311</td>\n",
       "      <td>0.748784</td>\n",
       "      <td>0.751702</td>\n",
       "      <td>0.763302</td>\n",
       "      <td>0.772398</td>\n",
       "      <td>0.786060</td>\n",
       "      <td>0.785323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.418973</td>\n",
       "      <td>0.108017</td>\n",
       "      <td>0.784175</td>\n",
       "      <td>0.778494</td>\n",
       "      <td>0.830247</td>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.756303</td>\n",
       "      <td>0.751628</td>\n",
       "      <td>0.776270</td>\n",
       "      <td>0.771426</td>\n",
       "      <td>0.792244</td>\n",
       "      <td>0.785692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.478042</td>\n",
       "      <td>0.075487</td>\n",
       "      <td>0.768526</td>\n",
       "      <td>0.779463</td>\n",
       "      <td>0.823310</td>\n",
       "      <td>0.830286</td>\n",
       "      <td>0.743034</td>\n",
       "      <td>0.752759</td>\n",
       "      <td>0.761247</td>\n",
       "      <td>0.773899</td>\n",
       "      <td>0.775945</td>\n",
       "      <td>0.785109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.466558</td>\n",
       "      <td>0.056030</td>\n",
       "      <td>0.771485</td>\n",
       "      <td>0.779363</td>\n",
       "      <td>0.821751</td>\n",
       "      <td>0.830527</td>\n",
       "      <td>0.744803</td>\n",
       "      <td>0.752587</td>\n",
       "      <td>0.762128</td>\n",
       "      <td>0.773346</td>\n",
       "      <td>0.781075</td>\n",
       "      <td>0.785474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  2.184534    0.077950       0.775268        0.778855  0.827496   0.829789   \n",
       "1  2.150011    0.124972       0.788132        0.777900  0.835867   0.828970   \n",
       "2  2.131026    0.095480       0.781732        0.778234  0.838677   0.828603   \n",
       "3  2.261994    0.120514       0.782011        0.778980  0.833092   0.829253   \n",
       "4  2.388806    0.178146       0.787298        0.777778  0.838794   0.828522   \n",
       "5  2.307807    0.125958       0.782333        0.777182  0.831703   0.829384   \n",
       "6  2.366782    0.121443       0.774514        0.778807  0.824350   0.830311   \n",
       "7  2.418973    0.108017       0.784175        0.778494  0.830247   0.829594   \n",
       "8  1.478042    0.075487       0.768526        0.779463  0.823310   0.830286   \n",
       "9  1.466558    0.056030       0.771485        0.779363  0.821751   0.830527   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.749226        0.752316        0.772817         0.773054   \n",
       "1       0.764706        0.750940        0.783452         0.771810   \n",
       "2       0.755860        0.751505        0.779882         0.771818   \n",
       "3       0.754533        0.751923        0.767836         0.772082   \n",
       "4       0.764485        0.750399        0.781833         0.770795   \n",
       "5       0.756966        0.750694        0.785601         0.771488   \n",
       "6       0.748784        0.751702        0.763302         0.772398   \n",
       "7       0.756303        0.751628        0.776270         0.771426   \n",
       "8       0.743034        0.752759        0.761247         0.773899   \n",
       "9       0.744803        0.752587        0.762128         0.773346   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.777734      0.784745  \n",
       "1     0.792869      0.784086  \n",
       "2     0.783591      0.784757  \n",
       "3     0.796719      0.786002  \n",
       "4     0.792840      0.784888  \n",
       "5     0.779093      0.782961  \n",
       "6     0.786060      0.785323  \n",
       "7     0.792244      0.785692  \n",
       "8     0.775945      0.785109  \n",
       "9     0.781075      0.785474  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           2.115453\n",
       "score_time         0.108400\n",
       "test_F1_Score      0.779547\n",
       "train_F1_Score     0.778506\n",
       "test_AUC           0.830529\n",
       "train_AUC          0.829524\n",
       "test_Accuracy      0.753870\n",
       "train_Accuracy     0.751645\n",
       "test_Precision     0.773437\n",
       "train_Precision    0.772212\n",
       "test_Recall        0.785817\n",
       "train_Recall       0.784904\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Logistic Regression with Shuffle cv object\n",
      "0:00:07.977410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from sklearn import metrics as mt\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "#modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "scoring = {'F1_Score': 'f1', 'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'Precision': 'precision', \\\n",
    "          'Recall': 'recall'}\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "logisticModel = make_pipeline(StandardScaler(),  LogisticRegression(penalty='l1', C=1.0, class_weight=None, random_state=999))\n",
    "\n",
    "start = datetime.now()\n",
    "scores_kfold = cross_validate(logisticModel , X, y=y , cv = kfold_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_kfold))\n",
    "display(pd.DataFrame(scores_kfold).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Kfold cv object\")\n",
    "print(end-start)\n",
    "print()\n",
    "\n",
    "start = datetime.now()\n",
    "scores_shuffle = cross_validate(logisticModel , X, y=y , cv = shuffle_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_shuffle))\n",
    "display(pd.DataFrame(scores_shuffle).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Shuffle cv object\")\n",
    "print(end-start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running poylnomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>696.325340</td>\n",
       "      <td>2.765477</td>\n",
       "      <td>0.945170</td>\n",
       "      <td>0.785199</td>\n",
       "      <td>0.601824</td>\n",
       "      <td>0.855696</td>\n",
       "      <td>0.896064</td>\n",
       "      <td>0.776721</td>\n",
       "      <td>0.896239</td>\n",
       "      <td>0.782627</td>\n",
       "      <td>0.999753</td>\n",
       "      <td>0.787788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>565.157112</td>\n",
       "      <td>2.596520</td>\n",
       "      <td>0.819712</td>\n",
       "      <td>0.787748</td>\n",
       "      <td>0.592195</td>\n",
       "      <td>0.860283</td>\n",
       "      <td>0.706260</td>\n",
       "      <td>0.781666</td>\n",
       "      <td>0.910434</td>\n",
       "      <td>0.793512</td>\n",
       "      <td>0.745432</td>\n",
       "      <td>0.782068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>646.139728</td>\n",
       "      <td>2.634551</td>\n",
       "      <td>0.496214</td>\n",
       "      <td>0.832336</td>\n",
       "      <td>0.632943</td>\n",
       "      <td>0.871197</td>\n",
       "      <td>0.440832</td>\n",
       "      <td>0.800909</td>\n",
       "      <td>0.346797</td>\n",
       "      <td>0.816900</td>\n",
       "      <td>0.871849</td>\n",
       "      <td>0.848367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>601.435450</td>\n",
       "      <td>2.529556</td>\n",
       "      <td>0.464406</td>\n",
       "      <td>0.822294</td>\n",
       "      <td>0.631675</td>\n",
       "      <td>0.875659</td>\n",
       "      <td>0.542358</td>\n",
       "      <td>0.802925</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.822458</td>\n",
       "      <td>0.349980</td>\n",
       "      <td>0.822130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>546.226694</td>\n",
       "      <td>2.693989</td>\n",
       "      <td>0.348553</td>\n",
       "      <td>0.825839</td>\n",
       "      <td>0.676370</td>\n",
       "      <td>0.860427</td>\n",
       "      <td>0.775935</td>\n",
       "      <td>0.788105</td>\n",
       "      <td>0.495430</td>\n",
       "      <td>0.805310</td>\n",
       "      <td>0.268849</td>\n",
       "      <td>0.847442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>643.732224</td>\n",
       "      <td>2.031840</td>\n",
       "      <td>0.622435</td>\n",
       "      <td>0.820816</td>\n",
       "      <td>0.759851</td>\n",
       "      <td>0.869019</td>\n",
       "      <td>0.686574</td>\n",
       "      <td>0.795134</td>\n",
       "      <td>0.683841</td>\n",
       "      <td>0.814652</td>\n",
       "      <td>0.571149</td>\n",
       "      <td>0.827074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>611.188166</td>\n",
       "      <td>2.688979</td>\n",
       "      <td>0.534449</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.666164</td>\n",
       "      <td>0.871099</td>\n",
       "      <td>0.630834</td>\n",
       "      <td>0.798353</td>\n",
       "      <td>0.637816</td>\n",
       "      <td>0.810788</td>\n",
       "      <td>0.459914</td>\n",
       "      <td>0.840023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>688.180470</td>\n",
       "      <td>1.603086</td>\n",
       "      <td>0.839953</td>\n",
       "      <td>0.791889</td>\n",
       "      <td>0.771620</td>\n",
       "      <td>0.857495</td>\n",
       "      <td>0.759345</td>\n",
       "      <td>0.779037</td>\n",
       "      <td>0.900915</td>\n",
       "      <td>0.788222</td>\n",
       "      <td>0.786718</td>\n",
       "      <td>0.795591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>471.425449</td>\n",
       "      <td>1.358225</td>\n",
       "      <td>0.884973</td>\n",
       "      <td>0.797228</td>\n",
       "      <td>0.830222</td>\n",
       "      <td>0.858635</td>\n",
       "      <td>0.819509</td>\n",
       "      <td>0.780708</td>\n",
       "      <td>0.831964</td>\n",
       "      <td>0.790278</td>\n",
       "      <td>0.945197</td>\n",
       "      <td>0.804301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>423.629601</td>\n",
       "      <td>0.783531</td>\n",
       "      <td>0.368894</td>\n",
       "      <td>0.828717</td>\n",
       "      <td>0.622348</td>\n",
       "      <td>0.863413</td>\n",
       "      <td>0.642778</td>\n",
       "      <td>0.792848</td>\n",
       "      <td>0.293532</td>\n",
       "      <td>0.814597</td>\n",
       "      <td>0.496320</td>\n",
       "      <td>0.843335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  696.325340    2.765477       0.945170        0.785199  0.601824   0.855696   \n",
       "1  565.157112    2.596520       0.819712        0.787748  0.592195   0.860283   \n",
       "2  646.139728    2.634551       0.496214        0.832336  0.632943   0.871197   \n",
       "3  601.435450    2.529556       0.464406        0.822294  0.631675   0.875659   \n",
       "4  546.226694    2.693989       0.348553        0.825839  0.676370   0.860427   \n",
       "5  643.732224    2.031840       0.622435        0.820816  0.759851   0.869019   \n",
       "6  611.188166    2.688979       0.534449        0.825147  0.666164   0.871099   \n",
       "7  688.180470    1.603086       0.839953        0.791889  0.771620   0.857495   \n",
       "8  471.425449    1.358225       0.884973        0.797228  0.830222   0.858635   \n",
       "9  423.629601    0.783531       0.368894        0.828717  0.622348   0.863413   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.896064        0.776721        0.896239         0.782627   \n",
       "1       0.706260        0.781666        0.910434         0.793512   \n",
       "2       0.440832        0.800909        0.346797         0.816900   \n",
       "3       0.542358        0.802925        0.690000         0.822458   \n",
       "4       0.775935        0.788105        0.495430         0.805310   \n",
       "5       0.686574        0.795134        0.683841         0.814652   \n",
       "6       0.630834        0.798353        0.637816         0.810788   \n",
       "7       0.759345        0.779037        0.900915         0.788222   \n",
       "8       0.819509        0.780708        0.831964         0.790278   \n",
       "9       0.642778        0.792848        0.293532         0.814597   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.999753      0.787788  \n",
       "1     0.745432      0.782068  \n",
       "2     0.871849      0.848367  \n",
       "3     0.349980      0.822130  \n",
       "4     0.268849      0.847442  \n",
       "5     0.571149      0.827074  \n",
       "6     0.459914      0.840023  \n",
       "7     0.786718      0.795591  \n",
       "8     0.945197      0.804301  \n",
       "9     0.496320      0.843335  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           589.344023\n",
       "score_time           2.168575\n",
       "test_F1_Score        0.632476\n",
       "train_F1_Score       0.811721\n",
       "test_AUC             0.678521\n",
       "train_AUC            0.864292\n",
       "test_Accuracy        0.690049\n",
       "train_Accuracy       0.789641\n",
       "test_Precision       0.668697\n",
       "train_Precision      0.803934\n",
       "test_Recall          0.649516\n",
       "train_Recall         0.819812\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Logistic Regression with Kfold cv object\n",
      "0:29:05.146339\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>599.361578</td>\n",
       "      <td>2.512565</td>\n",
       "      <td>0.800315</td>\n",
       "      <td>0.812515</td>\n",
       "      <td>0.851785</td>\n",
       "      <td>0.864261</td>\n",
       "      <td>0.775763</td>\n",
       "      <td>0.788960</td>\n",
       "      <td>0.792821</td>\n",
       "      <td>0.802510</td>\n",
       "      <td>0.807952</td>\n",
       "      <td>0.822772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>613.813326</td>\n",
       "      <td>2.223731</td>\n",
       "      <td>0.808485</td>\n",
       "      <td>0.811089</td>\n",
       "      <td>0.859143</td>\n",
       "      <td>0.863162</td>\n",
       "      <td>0.786378</td>\n",
       "      <td>0.786969</td>\n",
       "      <td>0.800235</td>\n",
       "      <td>0.800344</td>\n",
       "      <td>0.816907</td>\n",
       "      <td>0.822126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>719.080238</td>\n",
       "      <td>2.477588</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.810918</td>\n",
       "      <td>0.862007</td>\n",
       "      <td>0.862917</td>\n",
       "      <td>0.787705</td>\n",
       "      <td>0.787363</td>\n",
       "      <td>0.803260</td>\n",
       "      <td>0.801391</td>\n",
       "      <td>0.820452</td>\n",
       "      <td>0.820675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>631.250372</td>\n",
       "      <td>2.141786</td>\n",
       "      <td>0.810588</td>\n",
       "      <td>0.812057</td>\n",
       "      <td>0.855950</td>\n",
       "      <td>0.863685</td>\n",
       "      <td>0.786378</td>\n",
       "      <td>0.788075</td>\n",
       "      <td>0.794694</td>\n",
       "      <td>0.801247</td>\n",
       "      <td>0.827131</td>\n",
       "      <td>0.823163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>410.447711</td>\n",
       "      <td>2.488578</td>\n",
       "      <td>0.811047</td>\n",
       "      <td>0.811267</td>\n",
       "      <td>0.863257</td>\n",
       "      <td>0.862834</td>\n",
       "      <td>0.789695</td>\n",
       "      <td>0.787240</td>\n",
       "      <td>0.801335</td>\n",
       "      <td>0.801119</td>\n",
       "      <td>0.820998</td>\n",
       "      <td>0.821675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>639.324061</td>\n",
       "      <td>2.574531</td>\n",
       "      <td>0.801990</td>\n",
       "      <td>0.810330</td>\n",
       "      <td>0.851309</td>\n",
       "      <td>0.864012</td>\n",
       "      <td>0.779965</td>\n",
       "      <td>0.786650</td>\n",
       "      <td>0.809237</td>\n",
       "      <td>0.800207</td>\n",
       "      <td>0.794872</td>\n",
       "      <td>0.820713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>685.550675</td>\n",
       "      <td>2.823392</td>\n",
       "      <td>0.804493</td>\n",
       "      <td>0.812186</td>\n",
       "      <td>0.849843</td>\n",
       "      <td>0.864287</td>\n",
       "      <td>0.780628</td>\n",
       "      <td>0.788493</td>\n",
       "      <td>0.787423</td>\n",
       "      <td>0.802969</td>\n",
       "      <td>0.822321</td>\n",
       "      <td>0.821618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>706.072124</td>\n",
       "      <td>2.130784</td>\n",
       "      <td>0.795762</td>\n",
       "      <td>0.812512</td>\n",
       "      <td>0.848338</td>\n",
       "      <td>0.864507</td>\n",
       "      <td>0.769792</td>\n",
       "      <td>0.788911</td>\n",
       "      <td>0.789105</td>\n",
       "      <td>0.801922</td>\n",
       "      <td>0.802533</td>\n",
       "      <td>0.823386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>554.663549</td>\n",
       "      <td>1.456168</td>\n",
       "      <td>0.799127</td>\n",
       "      <td>0.812253</td>\n",
       "      <td>0.846864</td>\n",
       "      <td>0.864569</td>\n",
       "      <td>0.776205</td>\n",
       "      <td>0.788665</td>\n",
       "      <td>0.788793</td>\n",
       "      <td>0.803256</td>\n",
       "      <td>0.809735</td>\n",
       "      <td>0.821454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>413.419742</td>\n",
       "      <td>0.938464</td>\n",
       "      <td>0.798974</td>\n",
       "      <td>0.813192</td>\n",
       "      <td>0.846567</td>\n",
       "      <td>0.864553</td>\n",
       "      <td>0.774657</td>\n",
       "      <td>0.789378</td>\n",
       "      <td>0.786408</td>\n",
       "      <td>0.802625</td>\n",
       "      <td>0.811949</td>\n",
       "      <td>0.824041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  599.361578    2.512565       0.800315        0.812515  0.851785   0.864261   \n",
       "1  613.813326    2.223731       0.808485        0.811089  0.859143   0.863162   \n",
       "2  719.080238    2.477588       0.811765        0.810918  0.862007   0.862917   \n",
       "3  631.250372    2.141786       0.810588        0.812057  0.855950   0.863685   \n",
       "4  410.447711    2.488578       0.811047        0.811267  0.863257   0.862834   \n",
       "5  639.324061    2.574531       0.801990        0.810330  0.851309   0.864012   \n",
       "6  685.550675    2.823392       0.804493        0.812186  0.849843   0.864287   \n",
       "7  706.072124    2.130784       0.795762        0.812512  0.848338   0.864507   \n",
       "8  554.663549    1.456168       0.799127        0.812253  0.846864   0.864569   \n",
       "9  413.419742    0.938464       0.798974        0.813192  0.846567   0.864553   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.775763        0.788960        0.792821         0.802510   \n",
       "1       0.786378        0.786969        0.800235         0.800344   \n",
       "2       0.787705        0.787363        0.803260         0.801391   \n",
       "3       0.786378        0.788075        0.794694         0.801247   \n",
       "4       0.789695        0.787240        0.801335         0.801119   \n",
       "5       0.779965        0.786650        0.809237         0.800207   \n",
       "6       0.780628        0.788493        0.787423         0.802969   \n",
       "7       0.769792        0.788911        0.789105         0.801922   \n",
       "8       0.776205        0.788665        0.788793         0.803256   \n",
       "9       0.774657        0.789378        0.786408         0.802625   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.807952      0.822772  \n",
       "1     0.816907      0.822126  \n",
       "2     0.820452      0.820675  \n",
       "3     0.827131      0.823163  \n",
       "4     0.820998      0.821675  \n",
       "5     0.794872      0.820713  \n",
       "6     0.822321      0.821618  \n",
       "7     0.802533      0.823386  \n",
       "8     0.809735      0.821454  \n",
       "9     0.811949      0.824041  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           597.298337\n",
       "score_time           2.176759\n",
       "test_F1_Score        0.804255\n",
       "train_F1_Score       0.811832\n",
       "test_AUC             0.853506\n",
       "train_AUC            0.863879\n",
       "test_Accuracy        0.780716\n",
       "train_Accuracy       0.788070\n",
       "test_Precision       0.795331\n",
       "train_Precision      0.801759\n",
       "test_Recall          0.813485\n",
       "train_Recall         0.822162\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Logistic Regression with Shuffle cv object\n",
      "0:28:53.219431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "#modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "scoring = {'F1_Score': 'f1', 'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'Precision': 'precision', \\\n",
    "          'Recall': 'recall'}\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "logisticModel2 = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(),  LogisticRegression(penalty='l1', C=1.0, class_weight=None, random_state=999))\n",
    "\n",
    "start = datetime.now()\n",
    "scores_kfold2 = cross_validate(logisticModel2 , X, y=y , cv = kfold_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_kfold2))\n",
    "display(pd.DataFrame(scores_kfold2).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Kfold cv object\")\n",
    "print(end-start)\n",
    "print()\n",
    "\n",
    "start = datetime.now()\n",
    "scores_shuffle2 = cross_validate(logisticModel2 , X, y=y , cv = shuffle_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_shuffle2))\n",
    "display(pd.DataFrame(scores_shuffle2).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Shuffle cv object\")\n",
    "print(end-start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## polynomial of degree 3\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "#modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "scoring = {'F1_Score': 'f1', 'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'Precision': 'precision', \\\n",
    "          'Recall': 'recall'}\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "logisticModel3 = make_pipeline(PolynomialFeatures(degree=3), StandardScaler(),  LogisticRegression(penalty='l1', C=1.0, class_weight=None, random_state=999))\n",
    "\n",
    "start = datetime.now()\n",
    "scores_kfold3 = cross_validate(logisticModel3 , X, y=y , cv = kfold_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_kfold3))\n",
    "display(pd.DataFrame(scores_kfold3).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Kfold cv object\")\n",
    "print(end-start)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fir the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=999, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics as mt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "logisticModel_hsng = make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', C=1.0,\\\n",
    "                                                                   class_weight=None, random_state=999, \\\n",
    "                                                                   solver='liblinear'))\n",
    "\n",
    "# Fit the whole training dataset now, since validation would be done on additional dataset\n",
    "logisticModel_hsng.fit(X,y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month_aug has weight of -0.8218055880934713\n",
      "month_jun has weight of -0.5133078366224679\n",
      "age has weight of -0.34770575197344655\n",
      "month_jul has weight of -0.33219653576913705\n",
      "month_feb has weight of -0.30401987123143503\n",
      "job_student has weight of -0.2298849457383366\n",
      "month_jan has weight of -0.22251032200338852\n",
      "month_sep has weight of -0.21537644820959612\n",
      "month_oct has weight of -0.21508786998788285\n",
      "Subscribed_yes has weight of -0.21214732837628966\n",
      "marital_single has weight of -0.20485834964766605\n",
      "job_retired has weight of -0.20184179660375864\n",
      "job_unknown has weight of -0.17482566966737745\n",
      "month_mar has weight of -0.16929127133360752\n",
      "month_nov has weight of -0.1548033384929472\n",
      "month_dec has weight of -0.13777989156208167\n",
      "poutcome_success has weight of -0.12940030098315464\n",
      "day has weight of -0.11748156842113988\n",
      "job_housemaid has weight of -0.10266310732500522\n",
      "job_unemployed has weight of -0.0914933371853685\n",
      "contact_telephone has weight of -0.06990775131880754\n",
      "education_unknown has weight of -0.06908889225349335\n",
      "balance has weight of -0.06160846607376792\n",
      "job_self-employed has weight of -0.05478816511057268\n",
      "default_yes has weight of -0.03866118123443981\n",
      "poutcome_other has weight of -0.03355139251796191\n",
      "job_management has weight of -0.022941418672475247\n",
      "education_tertiary has weight of -0.017756816488902896\n",
      "job_entrepreneur has weight of -0.006165283082501696\n",
      "poutcome_unknown has weight of 0.0\n",
      "marital_married has weight of 0.0041355680086494935\n",
      "loan_yes has weight of 0.01581283932897791\n",
      "job_technician has weight of 0.016666225844844076\n",
      "job_services has weight of 0.02849243722019241\n",
      "education_secondary has weight of 0.03800683810887228\n",
      "previous has weight of 0.04202665532912485\n",
      "duration has weight of 0.07694250335968696\n",
      "campaign has weight of 0.11315624060102945\n",
      "job_blue-collar has weight of 0.12915086527664046\n",
      "contact_unknown has weight of 0.20524849208746798\n",
      "pdays has weight of 0.3212502833060009\n",
      "month_may has weight of 0.34290313815923523\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x648 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier_model = logisticModel_hsng.named_steps['logisticregression']\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(classifier_model.coef_.T, bankPromoModel_hsng_Df.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0])\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(20,9))\n",
    "weights = pd.Series(classifier_model.coef_[0],index=bankPromoModel_hsng_Df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning The Model Hyper Paramters Using Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'logisticregression__C': 75.43120063354607, 'logisticregression__penalty': 'l1', 'logisticregression__solver': 'liblinear'} with a score of 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "     'logisticregression__penalty' : ['l1', 'l2'],\n",
    "    'logisticregression__C' : np.logspace(-4, 4, 50),\n",
    "    'logisticregression__solver' : ['liblinear']\n",
    "   }\n",
    "\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression(random_state=999, class_weight=None)), \\\n",
    "                   param_grid = param_grid, cv = kfold_cv_object , verbose=False, n_jobs=-1, scoring=scoring, refit='Accuracy', \\\n",
    "                   return_train_score=True)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "grid.fit(X, y=y)\n",
    "  \n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'logisticregression__C': 0.0020235896477251557, 'logisticregression__penalty': 'l2', 'logisticregression__solver': 'liblinear'} with a score of 0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "     'logisticregression__penalty' : ['l1', 'l2'],\n",
    "    'logisticregression__C' : np.logspace(-4, 4, 50),\n",
    "    'logisticregression__solver' : ['liblinear']\n",
    "   }\n",
    "\n",
    "\n",
    "scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "# Create grid search object\n",
    "\n",
    "grid = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression(random_state=999, class_weight=None)), \\\n",
    "                   param_grid = param_grid, cv = shuffle_cv_object , verbose=False, n_jobs=-1, scoring=scoring, refit='Accuracy', \\\n",
    "                   return_train_score=True)\n",
    "\n",
    "# Fit on data\n",
    "\n",
    "grid.fit(X, y=y)\n",
    "  \n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection based on pearson coeff and Chi-Sq2 tests and top 18 imp features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'pdays']\n",
      "['job', 'education', 'loan', 'contact', 'month', 'poutcome', 'Subscribed']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 35 columns):\n",
      "age                    45211 non-null int64\n",
      "pdays                  45211 non-null int64\n",
      "Target                 45211 non-null int32\n",
      "job_blue-collar        45211 non-null uint8\n",
      "job_entrepreneur       45211 non-null uint8\n",
      "job_housemaid          45211 non-null uint8\n",
      "job_management         45211 non-null uint8\n",
      "job_retired            45211 non-null uint8\n",
      "job_self-employed      45211 non-null uint8\n",
      "job_services           45211 non-null uint8\n",
      "job_student            45211 non-null uint8\n",
      "job_technician         45211 non-null uint8\n",
      "job_unemployed         45211 non-null uint8\n",
      "job_unknown            45211 non-null uint8\n",
      "education_secondary    45211 non-null uint8\n",
      "education_tertiary     45211 non-null uint8\n",
      "education_unknown      45211 non-null uint8\n",
      "loan_yes               45211 non-null uint8\n",
      "contact_telephone      45211 non-null uint8\n",
      "contact_unknown        45211 non-null uint8\n",
      "month_aug              45211 non-null uint8\n",
      "month_dec              45211 non-null uint8\n",
      "month_feb              45211 non-null uint8\n",
      "month_jan              45211 non-null uint8\n",
      "month_jul              45211 non-null uint8\n",
      "month_jun              45211 non-null uint8\n",
      "month_mar              45211 non-null uint8\n",
      "month_may              45211 non-null uint8\n",
      "month_nov              45211 non-null uint8\n",
      "month_oct              45211 non-null uint8\n",
      "month_sep              45211 non-null uint8\n",
      "poutcome_other         45211 non-null uint8\n",
      "poutcome_success       45211 non-null uint8\n",
      "poutcome_unknown       45211 non-null uint8\n",
      "Subscribed_yes         45211 non-null uint8\n",
      "dtypes: int32(1), int64(2), uint8(32)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "source": [
    "# create dataframes on selected features\n",
    "## based on 2 methods 18 features\n",
    "bankPromoModel_hsng218_Df = bankPromo_df.copy()\n",
    "bankPromoModel_hsng218_Df['Target'] = bankPromoModel_hsng218_Df['housing'].apply(lambda resp : 1 if resp == \"yes\" else 0)\n",
    "bankPromoModel_hsng218_Df['Target'] = bankPromoModel_hsng218_Df['Target'].astype(np.int)\n",
    "\n",
    "# Delete the original 'housing' column\n",
    "del bankPromoModel_hsng218_Df['housing']\n",
    "del bankPromoModel_hsng218_Df['default']\n",
    "del bankPromoModel_hsng218_Df['previous']\n",
    "del bankPromoModel_hsng218_Df['marital']\n",
    "del bankPromoModel_hsng218_Df['duration']\n",
    "del bankPromoModel_hsng218_Df['day']\n",
    "del bankPromoModel_hsng218_Df['campaign']\n",
    "del bankPromoModel_hsng218_Df['balance']\n",
    "\n",
    "\n",
    "# Get the unique values(Levels) for categorical variables.\n",
    "# List to hold names of categorical variables\n",
    "categoricalVars218 = list()\n",
    "# List to hold names of numerical variables\n",
    "numericalVars218 = list()\n",
    "\n",
    "for colName in bankPromoModel_hsng218_Df.columns:\n",
    "    if bankPromoModel_hsng218_Df[colName].dtype == np.int64:\n",
    "        numericalVars218.append(colName)\n",
    "    elif bankPromoModel_hsng218_Df[colName].dtype == np.object:\n",
    "        categoricalVars218.append(colName)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "# Remove Target column from final categorical Var list\n",
    "#categoricalVars618.remove('Target')\n",
    "\n",
    "print(numericalVars218)\n",
    "print(categoricalVars218)\n",
    "\n",
    "# Convert all categorical variables to corresponding indicator variables\n",
    "for categoricalVar in categoricalVars218:\n",
    "    tmpDf = pd.DataFrame()\n",
    "    # Remove 1st class level to avoid multicollinearity\n",
    "    tmpDf = pd.get_dummies(bankPromoModel_hsng218_Df[categoricalVar], prefix=categoricalVar, drop_first=True)\n",
    "    bankPromoModel_hsng218_Df = pd.concat((bankPromoModel_hsng218_Df, tmpDf), axis=1)\n",
    "\n",
    "# Now remove the original categorical vars since indicator variables are created from them.\n",
    "bankPromoModel_hsng218_Df.drop(categoricalVars218, inplace=True, axis=1)\n",
    "bankPromoModel_hsng218_Df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create 10 Splits Stratified Cross Validation Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Test Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if 'Target' in bankPromoModel_hsng218_Df:\n",
    "    y218 = bankPromoModel_hsng218_Df['Target'].values # get the labels we want\n",
    "    del bankPromoModel_hsng218_Df['Target']        # get rid of the class label\n",
    "    X218 = bankPromoModel_hsng218_Df.values           # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.550702</td>\n",
       "      <td>0.085951</td>\n",
       "      <td>0.944477</td>\n",
       "      <td>0.739109</td>\n",
       "      <td>0.578961</td>\n",
       "      <td>0.810300</td>\n",
       "      <td>0.894958</td>\n",
       "      <td>0.729042</td>\n",
       "      <td>0.897180</td>\n",
       "      <td>0.737312</td>\n",
       "      <td>0.997038</td>\n",
       "      <td>0.740915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.441770</td>\n",
       "      <td>0.096944</td>\n",
       "      <td>0.842384</td>\n",
       "      <td>0.745253</td>\n",
       "      <td>0.595578</td>\n",
       "      <td>0.819401</td>\n",
       "      <td>0.736784</td>\n",
       "      <td>0.738216</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.751471</td>\n",
       "      <td>0.785185</td>\n",
       "      <td>0.739137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.300789</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.489116</td>\n",
       "      <td>0.805937</td>\n",
       "      <td>0.617726</td>\n",
       "      <td>0.837935</td>\n",
       "      <td>0.403008</td>\n",
       "      <td>0.765421</td>\n",
       "      <td>0.335149</td>\n",
       "      <td>0.777773</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.836216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.440767</td>\n",
       "      <td>0.099941</td>\n",
       "      <td>0.075350</td>\n",
       "      <td>0.793940</td>\n",
       "      <td>0.513998</td>\n",
       "      <td>0.845366</td>\n",
       "      <td>0.429993</td>\n",
       "      <td>0.772352</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.797150</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>0.790756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.331774</td>\n",
       "      <td>0.088950</td>\n",
       "      <td>0.268366</td>\n",
       "      <td>0.787144</td>\n",
       "      <td>0.664718</td>\n",
       "      <td>0.815946</td>\n",
       "      <td>0.784119</td>\n",
       "      <td>0.741214</td>\n",
       "      <td>0.549080</td>\n",
       "      <td>0.768108</td>\n",
       "      <td>0.177579</td>\n",
       "      <td>0.807147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.199850</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>0.584096</td>\n",
       "      <td>0.781361</td>\n",
       "      <td>0.764440</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.678390</td>\n",
       "      <td>0.751389</td>\n",
       "      <td>0.703653</td>\n",
       "      <td>0.779709</td>\n",
       "      <td>0.499267</td>\n",
       "      <td>0.783019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.121871</td>\n",
       "      <td>0.094946</td>\n",
       "      <td>0.409184</td>\n",
       "      <td>0.788911</td>\n",
       "      <td>0.691436</td>\n",
       "      <td>0.833053</td>\n",
       "      <td>0.618668</td>\n",
       "      <td>0.756550</td>\n",
       "      <td>0.714970</td>\n",
       "      <td>0.775135</td>\n",
       "      <td>0.286606</td>\n",
       "      <td>0.803185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.972302</td>\n",
       "      <td>0.048973</td>\n",
       "      <td>0.888950</td>\n",
       "      <td>0.749460</td>\n",
       "      <td>0.773183</td>\n",
       "      <td>0.816797</td>\n",
       "      <td>0.820173</td>\n",
       "      <td>0.737626</td>\n",
       "      <td>0.881365</td>\n",
       "      <td>0.756383</td>\n",
       "      <td>0.896666</td>\n",
       "      <td>0.742663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.020419</td>\n",
       "      <td>0.064963</td>\n",
       "      <td>0.897564</td>\n",
       "      <td>0.756787</td>\n",
       "      <td>0.833758</td>\n",
       "      <td>0.816321</td>\n",
       "      <td>0.840964</td>\n",
       "      <td>0.736471</td>\n",
       "      <td>0.851812</td>\n",
       "      <td>0.748788</td>\n",
       "      <td>0.948509</td>\n",
       "      <td>0.764959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.177328</td>\n",
       "      <td>0.067977</td>\n",
       "      <td>0.370974</td>\n",
       "      <td>0.795416</td>\n",
       "      <td>0.644534</td>\n",
       "      <td>0.824746</td>\n",
       "      <td>0.624198</td>\n",
       "      <td>0.751241</td>\n",
       "      <td>0.286286</td>\n",
       "      <td>0.777839</td>\n",
       "      <td>0.526814</td>\n",
       "      <td>0.813805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  1.550702    0.085951       0.944477        0.739109  0.578961   0.810300   \n",
       "1  1.441770    0.096944       0.842384        0.745253  0.595578   0.819401   \n",
       "2  1.300789    0.088983       0.489116        0.805937  0.617726   0.837935   \n",
       "3  1.440767    0.099941       0.075350        0.793940  0.513998   0.845366   \n",
       "4  1.331774    0.088950       0.268366        0.787144  0.664718   0.815946   \n",
       "5  1.199850    0.074965       0.584096        0.781361  0.764440   0.828571   \n",
       "6  1.121871    0.094946       0.409184        0.788911  0.691436   0.833053   \n",
       "7  2.972302    0.048973       0.888950        0.749460  0.773183   0.816797   \n",
       "8  1.020419    0.064963       0.897564        0.756787  0.833758   0.816321   \n",
       "9  1.177328    0.067977       0.370974        0.795416  0.644534   0.824746   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.894958        0.729042        0.897180         0.737312   \n",
       "1       0.736784        0.738216        0.908571         0.751471   \n",
       "2       0.403008        0.765421        0.335149         0.777773   \n",
       "3       0.429993        0.772352        0.468750         0.797150   \n",
       "4       0.784119        0.741214        0.549080         0.768108   \n",
       "5       0.678390        0.751389        0.703653         0.779709   \n",
       "6       0.618668        0.756550        0.714970         0.775135   \n",
       "7       0.820173        0.737626        0.881365         0.756383   \n",
       "8       0.840964        0.736471        0.851812         0.748788   \n",
       "9       0.624198        0.751241        0.286286         0.777839   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.997038      0.740915  \n",
       "1     0.785185      0.739137  \n",
       "2     0.904762      0.836216  \n",
       "3     0.040968      0.790756  \n",
       "4     0.177579      0.807147  \n",
       "5     0.499267      0.783019  \n",
       "6     0.286606      0.803185  \n",
       "7     0.896666      0.742663  \n",
       "8     0.948509      0.764959  \n",
       "9     0.526814      0.813805  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           1.455757\n",
       "score_time         0.081259\n",
       "test_F1_Score      0.577046\n",
       "train_F1_Score     0.774332\n",
       "test_AUC           0.667833\n",
       "train_AUC          0.824844\n",
       "test_Accuracy      0.683126\n",
       "train_Accuracy     0.747952\n",
       "test_Precision     0.659682\n",
       "train_Precision    0.766967\n",
       "test_Recall        0.606339\n",
       "train_Recall       0.782180\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Logistic Regression with Kfold cv object\n",
      "0:00:10.290283\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_F1_Score</th>\n",
       "      <th>train_F1_Score</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>test_Accuracy</th>\n",
       "      <th>train_Accuracy</th>\n",
       "      <th>test_Precision</th>\n",
       "      <th>train_Precision</th>\n",
       "      <th>test_Recall</th>\n",
       "      <th>train_Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.683035</td>\n",
       "      <td>0.091948</td>\n",
       "      <td>0.774892</td>\n",
       "      <td>0.773085</td>\n",
       "      <td>0.822790</td>\n",
       "      <td>0.824528</td>\n",
       "      <td>0.747015</td>\n",
       "      <td>0.745140</td>\n",
       "      <td>0.767043</td>\n",
       "      <td>0.765216</td>\n",
       "      <td>0.782903</td>\n",
       "      <td>0.781119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.558111</td>\n",
       "      <td>0.100942</td>\n",
       "      <td>0.775144</td>\n",
       "      <td>0.773766</td>\n",
       "      <td>0.829510</td>\n",
       "      <td>0.823909</td>\n",
       "      <td>0.750332</td>\n",
       "      <td>0.745582</td>\n",
       "      <td>0.770693</td>\n",
       "      <td>0.765568</td>\n",
       "      <td>0.779647</td>\n",
       "      <td>0.782142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.660052</td>\n",
       "      <td>0.107941</td>\n",
       "      <td>0.779394</td>\n",
       "      <td>0.773131</td>\n",
       "      <td>0.834068</td>\n",
       "      <td>0.823295</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.745091</td>\n",
       "      <td>0.773313</td>\n",
       "      <td>0.764701</td>\n",
       "      <td>0.785573</td>\n",
       "      <td>0.781749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.748003</td>\n",
       "      <td>0.176416</td>\n",
       "      <td>0.777734</td>\n",
       "      <td>0.773685</td>\n",
       "      <td>0.827675</td>\n",
       "      <td>0.824061</td>\n",
       "      <td>0.750111</td>\n",
       "      <td>0.745263</td>\n",
       "      <td>0.764797</td>\n",
       "      <td>0.764719</td>\n",
       "      <td>0.791116</td>\n",
       "      <td>0.782864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.606099</td>\n",
       "      <td>0.103942</td>\n",
       "      <td>0.778264</td>\n",
       "      <td>0.773096</td>\n",
       "      <td>0.832877</td>\n",
       "      <td>0.823397</td>\n",
       "      <td>0.753649</td>\n",
       "      <td>0.744550</td>\n",
       "      <td>0.770292</td>\n",
       "      <td>0.764419</td>\n",
       "      <td>0.786404</td>\n",
       "      <td>0.781973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.491657</td>\n",
       "      <td>0.107939</td>\n",
       "      <td>0.775050</td>\n",
       "      <td>0.773234</td>\n",
       "      <td>0.824467</td>\n",
       "      <td>0.824407</td>\n",
       "      <td>0.748784</td>\n",
       "      <td>0.745312</td>\n",
       "      <td>0.778131</td>\n",
       "      <td>0.764716</td>\n",
       "      <td>0.771992</td>\n",
       "      <td>0.781943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.512645</td>\n",
       "      <td>0.086950</td>\n",
       "      <td>0.772781</td>\n",
       "      <td>0.773454</td>\n",
       "      <td>0.819409</td>\n",
       "      <td>0.825066</td>\n",
       "      <td>0.745245</td>\n",
       "      <td>0.745042</td>\n",
       "      <td>0.756955</td>\n",
       "      <td>0.765166</td>\n",
       "      <td>0.789283</td>\n",
       "      <td>0.781923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.498651</td>\n",
       "      <td>0.079954</td>\n",
       "      <td>0.773307</td>\n",
       "      <td>0.774222</td>\n",
       "      <td>0.824163</td>\n",
       "      <td>0.824513</td>\n",
       "      <td>0.744582</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.767134</td>\n",
       "      <td>0.764716</td>\n",
       "      <td>0.779581</td>\n",
       "      <td>0.783967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.049978</td>\n",
       "      <td>0.766169</td>\n",
       "      <td>0.774012</td>\n",
       "      <td>0.819647</td>\n",
       "      <td>0.824888</td>\n",
       "      <td>0.740159</td>\n",
       "      <td>0.746025</td>\n",
       "      <td>0.758173</td>\n",
       "      <td>0.766635</td>\n",
       "      <td>0.774336</td>\n",
       "      <td>0.781532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.046974</td>\n",
       "      <td>0.767156</td>\n",
       "      <td>0.774154</td>\n",
       "      <td>0.816886</td>\n",
       "      <td>0.825277</td>\n",
       "      <td>0.740380</td>\n",
       "      <td>0.745902</td>\n",
       "      <td>0.759027</td>\n",
       "      <td>0.765674</td>\n",
       "      <td>0.775461</td>\n",
       "      <td>0.782824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_F1_Score  train_F1_Score  test_AUC  train_AUC  \\\n",
       "0  1.683035    0.091948       0.774892        0.773085  0.822790   0.824528   \n",
       "1  1.558111    0.100942       0.775144        0.773766  0.829510   0.823909   \n",
       "2  1.660052    0.107941       0.779394        0.773131  0.834068   0.823295   \n",
       "3  1.748003    0.176416       0.777734        0.773685  0.827675   0.824061   \n",
       "4  1.606099    0.103942       0.778264        0.773096  0.832877   0.823397   \n",
       "5  1.491657    0.107939       0.775050        0.773234  0.824467   0.824407   \n",
       "6  1.512645    0.086950       0.772781        0.773454  0.819409   0.825066   \n",
       "7  1.498651    0.079954       0.773307        0.774222  0.824163   0.824513   \n",
       "8  0.853510    0.049978       0.766169        0.774012  0.819647   0.824888   \n",
       "9  0.822529    0.046974       0.767156        0.774154  0.816886   0.825277   \n",
       "\n",
       "   test_Accuracy  train_Accuracy  test_Precision  train_Precision  \\\n",
       "0       0.747015        0.745140        0.767043         0.765216   \n",
       "1       0.750332        0.745582        0.770693         0.765568   \n",
       "2       0.751880        0.745091        0.773313         0.764701   \n",
       "3       0.750111        0.745263        0.764797         0.764719   \n",
       "4       0.753649        0.744550        0.770292         0.764419   \n",
       "5       0.748784        0.745312        0.778131         0.764716   \n",
       "6       0.745245        0.745042        0.756955         0.765166   \n",
       "7       0.744582        0.746000        0.767134         0.764716   \n",
       "8       0.740159        0.746025        0.758173         0.766635   \n",
       "9       0.740380        0.745902        0.759027         0.765674   \n",
       "\n",
       "   test_Recall  train_Recall  \n",
       "0     0.782903      0.781119  \n",
       "1     0.779647      0.782142  \n",
       "2     0.785573      0.781749  \n",
       "3     0.791116      0.782864  \n",
       "4     0.786404      0.781973  \n",
       "5     0.771992      0.781943  \n",
       "6     0.789283      0.781923  \n",
       "7     0.779581      0.783967  \n",
       "8     0.774336      0.781532  \n",
       "9     0.775461      0.782824  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fit_time           1.443429\n",
       "score_time         0.095298\n",
       "test_F1_Score      0.773989\n",
       "train_F1_Score     0.773584\n",
       "test_AUC           0.825149\n",
       "train_AUC          0.824334\n",
       "test_Accuracy      0.747214\n",
       "train_Accuracy     0.745391\n",
       "test_Precision     0.766556\n",
       "train_Precision    0.765153\n",
       "test_Recall        0.781630\n",
       "train_Recall       0.782204\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for Logistic Regression with Shuffle cv object\n",
      "0:00:05.943616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from sklearn import metrics as mt\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "# Initialize an Empty Dataframe to store Model performance Stats\n",
    "#modelPerformanceMetrices = pd.DataFrame(columns=['Accuracy', 'AuC', 'F1 Score', 'Precision', 'Recall'])\n",
    "scoring = {'F1_Score': 'f1', 'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score), 'Precision': 'precision', \\\n",
    "          'Recall': 'recall'}\n",
    "\n",
    "# Standardize the features first, since standardizing the features could lead to\n",
    "# gradient desent algo to converge faster and then run logistic regression model\n",
    "\n",
    "logisticModel = make_pipeline(StandardScaler(),  LogisticRegression(penalty='l1', C=1.0, class_weight=None, random_state=999))\n",
    "\n",
    "start = datetime.now()\n",
    "scores_kfold218 = cross_validate(logisticModel , X218, y=y218 , cv = kfold_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_kfold218))\n",
    "display(pd.DataFrame(scores_kfold218).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Kfold cv object\")\n",
    "print(end-start)\n",
    "print()\n",
    "\n",
    "start = datetime.now()\n",
    "scores_shuffle218 = cross_validate(logisticModel , X218, y=y218 , cv = shuffle_cv_object , n_jobs = -1 , scoring = scoring)\n",
    "print()\n",
    "display(pd.DataFrame(scores_shuffle218))\n",
    "display(pd.DataFrame(scores_shuffle218).mean())\n",
    "end = datetime.now()\n",
    "print(\"time taken for Logistic Regression with Shuffle cv object\")\n",
    "print(end-start)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
